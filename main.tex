\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{esint}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
% \usepackage{mathabx}
\usepackage[linesnumbered , ruled , vlined]{algorithm2e}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{floatrow}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage[hypcap=false]{caption}
\usepackage{hyperref}
\usepackage{subfig}
\usepackage{tikz}
\usepackage{hyperref}

\pagestyle{fancy}
\fancyhf{}
\lhead{200050107, 200050130, 200050154, 200050157}
\rhead{CS337}
\cfoot{\thepage}

\newcommand{\B}[1]{\textbf{#1}}
\newcommand{\I}[1]{\textit{#1}}

\title{\textbf{CS337 Course Project \\ Face Recognition}}
\author{Pranjal Kushwaha, Shashwat Garg, Vedang Asgaonkar, Virendra Kabra}
\date{Autumn 2022}

\begin{document}
\begin{sloppypar}       % for overfull, etc.

    \maketitle
    \tableofcontents

    \newpage

    \section{Dataset}
    We use the Labelled Faces in the Wild (LFW) dataset. It has been taken from \href{https://www.kaggle.com/datasets/jessicali9530/lfw-dataset}{Kaggle}. There is an uneven distribution of images, with only 10 people having at least 53 images. Thus, we train the model to classify these 10 people, with our dataset containing 53 images for each. The train-test split is 80-20, and the train set is further split to create the validation set.

    \section{Model Architecture}

    We use a Convolutional Neural Network (CNN) to create a multi-class image classifier. The network is inspired from FaceNet \cite{facenet}.

    \begin{center}
        \begin{table}[!h]
            \begin{tabular}{|c|c|c|c|c|}
                \hline
                \B{Layer} & \B{In} & \B{Out} & \B{Kernel} & \B{Params}\\
                \hline \hline
                conv1 & $250\times 250\times 3$ & $123\times 123\times 64$ & $7\times 7\times 3, 2$ & 9K\\
                batchnorm1 & $123\times 123\times 64$ & $123\times 123\times 64$ & & 128\\
                relu1 & $123\times 123\times 64$ & $123\times 123\times 64$ & & 0\\
                maxpool1 & $123\times 123\times 64$ & $61\times 61\times 64$ & $2\times 2\times 64, 2$ & 0\\
                dropout1 & $61\times 61\times 64$ & $61\times 61\times 64$ & & 0\\
                \hline
                conv2 & $61\times 61\times 64$ & $61\times 61\times 128$ & $3\times 3\times 64, 1$ & 74K\\
                batchnorm2 & $61\times 61\times 128$ & $61\times 61\times 128$ & & 256\\
                relu2 & $61\times 61\times 128$ & $61\times 61\times 128$ & & 0\\
                maxpool2 & $61\times 61\times 128$ & $30\times 30\times 128$ & $2\times 2\times 128, 2$ & 0\\
                dropout2 & $30\times 30\times 128$ & $30\times 30\times 128$ & & 0\\
                \hline
                conv3 & $30\times 30\times 128$ & $30\times 30\times 256$ & $3\times 3\times 128, 1$ & 295K\\
                batchnorm3 & $30\times 30\times 256$ & $30\times 30\times 256$ & & 512\\
                relu3 & $30\times 30\times 256$ & $30\times 30\times 256$ & & 0\\
                maxpool3 & $30\times 30\times 256$ & $15\times 15\times 256$ & $2\times 2\times 128, 2$ & 0\\
                dropout3 & $15\times 15\times 256$ & $15\times 15\times 256$ & & 0\\
                \hline
                conv4 & $15\times 15\times 256$ & $15\times 15\times 64$ & $3\times 3\times 256, 1$ & 148K\\
                batchnorm4 & $15\times 15\times 64$ & $15\times 15\times 64$ & & 128\\
                relu4 & $15\times 15\times 64$ & $15\times 15\times 64$ & & 0\\
                dropout4 & $15\times 15\times 64$ & $15\times 15\times 64$ & & 0\\
                \hline
                flatten & $15\times 15\times 64$ & 14400 & & 0\\
                \hline
                fc1 & 14400 & 1024 & & 14M\\
                relu5 & 1024 & 1024 & & 0\\
                dropout5 & 1024 & 1024 & & 0\\
                \hline
                fc2 & 1024 & 64 & & 66K\\
                relu5 & 64 & 64 & & 0\\
                dropout5 & 64 & 64 & & 0\\
                \hline
                fc3 & 64 & 10 & & 650\\
                \hline \hline
                Total & & & & 15M\\
                \hline
            \end{tabular}
            \caption{\label{table-1}Model Architecture}
        \end{table}
    \end{center}

    \section{Interpretable Architecture}
    Alongside the categorical cross entropy loss, we use the triplet loss as introduced in FaceNet \cite{facenet}. We view the neural network as an encoder-decoder combination.
    The network upto its penultimate layer is the encoder and the last layer is the decoder. Thus the encoder produces a $64$ dimensional latent representation of each image. 
    The triplet loss encourages clustering in this latent space i.e. the latent representations of images in the same class are close together.
    \par The triplet loss $L_T$ defined on a batch of latent vectors $F$ is given by 
    \begin{equation*}
        L_T(F) = \sum_{f_a \in F} \sum_{f_p \in F_p(f_a)} \sum_{f_n \in F_n(f_a)} [ ||f_a - f_p||^2 - ||f_a - f_n||^2 + \alpha ]_+
    \end{equation*}
    where $F_p(f_a)$ is the subset of $F$ having the same class as $f_a$, $F_n(f_a)$ is its complement and $\alpha$ here is margin which we set to $1$.
    \par Thus, the loss forces latent vectors of the same class to reduce their Euclidean distance, while increasing the Euclidean distance between vectors of different classes.
    \par By employing the triplet loss, the latent vectors get clustered, as measured by the ratio of mean squared distance within a cluster to the mean squared distance between clusters.
    This ratio goes down to roughly $0.3$ by employing the triplet loss.

    \section{Adversarial attack on the CNN}
    We demonstrate an attack on the CNN, in which we provide images which are visually similar to the original image, but get misclassified. To that end, we train a generator which is designed to fool
    the model. The generator adds a bias to each pixel of the image. We first freeze the model weights. The output of the generator is passed to the model as input. We train this composite system using a loss 
    which is negative of the cross entropy loss. This encourages the generator to produce images which are misclassified. We also add a regularizer, to encourage similarity of the generated image with the original one.
    \begin{center}
        \ffigbox [ 1 \textwidth ]{ \caption{Adversarial attack} } { \includegraphics [ width =1 \textwidth ] {attack.png}}   
    \end{center}
    \par Observations:
    \begin{itemize}
        \item By adding a $l_2$ regularizer with $\lambda = 20000$, the generator brings down the model's test accuracy to $35$\% with minimal loss of visual similarity.
        \item $l_1$ regularizer does not do perform as good as $l_2$. Trained with $\lambda = 100$, it brings down the test accuracy to $38$\% but ends up adding too much noise.
    \end{itemize}
    \begin{center}
        \ffigbox [ 0.6 \textwidth ]{ \caption{l1 regularizer} } { \includegraphics [ width =0.6 \textwidth ] {l1.png}}   
    \end{center}
    \begin{center}
        \ffigbox [ 0.6 \textwidth ]{ \caption{l2 regularizer} } { \includegraphics [ width =0.6 \textwidth ] {l2.png}}   
    \end{center}
    \bibliographystyle{abbrv}
    \bibliography{biblio}

\end{sloppypar}
\end{document}