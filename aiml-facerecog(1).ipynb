{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d0eab29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:49.515439Z",
     "iopub.status.busy": "2022-11-23T04:35:49.514982Z",
     "iopub.status.idle": "2022-11-23T04:35:50.496482Z",
     "shell.execute_reply": "2022-11-23T04:35:50.495369Z"
    },
    "papermill": {
     "duration": 0.991732,
     "end_time": "2022-11-23T04:35:50.499105",
     "exception": false,
     "start_time": "2022-11-23T04:35:49.507373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 23 04:35:50 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   34C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc856f4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:50.510719Z",
     "iopub.status.busy": "2022-11-23T04:35:50.510410Z",
     "iopub.status.idle": "2022-11-23T04:35:53.481225Z",
     "shell.execute_reply": "2022-11-23T04:35:53.480280Z"
    },
    "papermill": {
     "duration": 2.978817,
     "end_time": "2022-11-23T04:35:53.483175",
     "exception": false,
     "start_time": "2022-11-23T04:35:50.504358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee18de6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:53.495597Z",
     "iopub.status.busy": "2022-11-23T04:35:53.494408Z",
     "iopub.status.idle": "2022-11-23T04:35:53.501039Z",
     "shell.execute_reply": "2022-11-23T04:35:53.500048Z"
    },
    "papermill": {
     "duration": 0.014947,
     "end_time": "2022-11-23T04:35:53.503123",
     "exception": false,
     "start_time": "2022-11-23T04:35:53.488176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlfw/\\n    csv files\\n    lfw-deepfunneled/\\ndata/\\n    train/\\n    val/\\n    test/\\nnotebook\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "lfw/\n",
    "    csv files\n",
    "    lfw-deepfunneled/\n",
    "data/\n",
    "    train/\n",
    "    val/\n",
    "    test/\n",
    "notebook\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24dc6df9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:53.513960Z",
     "iopub.status.busy": "2022-11-23T04:35:53.513707Z",
     "iopub.status.idle": "2022-11-23T04:35:53.524398Z",
     "shell.execute_reply": "2022-11-23T04:35:53.523466Z"
    },
    "papermill": {
     "duration": 0.01849,
     "end_time": "2022-11-23T04:35:53.526501",
     "exception": false,
     "start_time": "2022-11-23T04:35:53.508011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['src', 'lib', 'input', 'working']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "device = torch.device(device)\n",
    "os.getcwd()\n",
    "os.listdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1585de96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:53.537822Z",
     "iopub.status.busy": "2022-11-23T04:35:53.537571Z",
     "iopub.status.idle": "2022-11-23T04:35:53.542746Z",
     "shell.execute_reply": "2022-11-23T04:35:53.541938Z"
    },
    "papermill": {
     "duration": 0.013139,
     "end_time": "2022-11-23T04:35:53.544822",
     "exception": false,
     "start_time": "2022-11-23T04:35:53.531683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = '../input/lfw-dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77fcb018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:53.555860Z",
     "iopub.status.busy": "2022-11-23T04:35:53.555602Z",
     "iopub.status.idle": "2022-11-23T04:35:53.608844Z",
     "shell.execute_reply": "2022-11-23T04:35:53.608029Z"
    },
    "papermill": {
     "duration": 0.061353,
     "end_time": "2022-11-23T04:35:53.611117",
     "exception": false,
     "start_time": "2022-11-23T04:35:53.549764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lfw_allnames = pd.read_csv(data_folder+\"lfw_allnames.csv\")\n",
    "\n",
    "image_paths = lfw_allnames.loc[lfw_allnames.index.repeat(lfw_allnames['images'])]\n",
    "image_paths['image_path'] = 1 + image_paths.groupby('name').cumcount()\n",
    "image_paths['image_path'] = image_paths.image_path.apply(lambda x: str(x).zfill(4))\n",
    "image_paths['image_path'] = image_paths.name + \"/\" + image_paths.name + \"_\" + image_paths.image_path + \".jpg\"\n",
    "image_paths = image_paths.drop(\"images\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bb72abd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:53.622319Z",
     "iopub.status.busy": "2022-11-23T04:35:53.622037Z",
     "iopub.status.idle": "2022-11-23T04:35:53.638919Z",
     "shell.execute_reply": "2022-11-23T04:35:53.638111Z"
    },
    "papermill": {
     "duration": 0.024754,
     "end_time": "2022-11-23T04:35:53.640908",
     "exception": false,
     "start_time": "2022-11-23T04:35:53.616154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George_W_Bush                530\n",
      "Colin_Powell                 236\n",
      "Tony_Blair                   144\n",
      "Donald_Rumsfeld              121\n",
      "Gerhard_Schroeder            109\n",
      "Ariel_Sharon                  77\n",
      "Hugo_Chavez                   71\n",
      "Junichiro_Koizumi             60\n",
      "Jean_Chretien                 55\n",
      "John_Ashcroft                 53\n",
      "Jacques_Chirac                52\n",
      "Serena_Williams               52\n",
      "Vladimir_Putin                49\n",
      "Luiz_Inacio_Lula_da_Silva     48\n",
      "Gloria_Macapagal_Arroyo       44\n",
      "Jennifer_Capriati             42\n",
      "Arnold_Schwarzenegger         42\n",
      "Laura_Bush                    41\n",
      "Lleyton_Hewitt                41\n",
      "Hans_Blix                     39\n",
      "Alejandro_Toledo              39\n",
      "Nestor_Kirchner               37\n",
      "Andre_Agassi                  36\n",
      "Alvaro_Uribe                  35\n",
      "Silvio_Berlusconi             33\n",
      "Name: name, dtype: int64\n",
      "['George_W_Bush', 'Colin_Powell', 'Tony_Blair', 'Donald_Rumsfeld', 'Gerhard_Schroeder', 'Ariel_Sharon', 'Hugo_Chavez', 'Junichiro_Koizumi', 'Jean_Chretien', 'John_Ashcroft', 'Jacques_Chirac', 'Serena_Williams', 'Vladimir_Putin', 'Luiz_Inacio_Lula_da_Silva', 'Gloria_Macapagal_Arroyo', 'Jennifer_Capriati', 'Arnold_Schwarzenegger', 'Laura_Bush', 'Lleyton_Hewitt', 'Hans_Blix', 'Alejandro_Toledo', 'Nestor_Kirchner', 'Andre_Agassi', 'Alvaro_Uribe', 'Silvio_Berlusconi'] [530, 236, 144, 121, 109, 77, 71, 60, 55, 53, 52, 52, 49, 48, 44, 42, 42, 41, 41, 39, 39, 37, 36, 35, 33]\n"
     ]
    }
   ],
   "source": [
    "num_people = 25\n",
    "print(image_paths['name'].value_counts()[:num_people])\n",
    "list_people = list(image_paths['name'].value_counts()[:num_people].keys())\n",
    "list_num_images = list(image_paths['name'].value_counts()[:num_people])\n",
    "print(list_people, list_num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7dd3b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:53.654801Z",
     "iopub.status.busy": "2022-11-23T04:35:53.653211Z",
     "iopub.status.idle": "2022-11-23T04:35:53.697583Z",
     "shell.execute_reply": "2022-11-23T04:35:53.696019Z"
    },
    "papermill": {
     "duration": 0.054277,
     "end_time": "2022-11-23T04:35:53.700191",
     "exception": false,
     "start_time": "2022-11-23T04:35:53.645914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name                                    image_path\n",
      "1871      George_W_Bush          George_W_Bush/George_W_Bush_0173.jpg\n",
      "1871      George_W_Bush          George_W_Bush/George_W_Bush_0310.jpg\n",
      "1871      George_W_Bush          George_W_Bush/George_W_Bush_0401.jpg\n",
      "1871      George_W_Bush          George_W_Bush/George_W_Bush_0375.jpg\n",
      "1871      George_W_Bush          George_W_Bush/George_W_Bush_0108.jpg\n",
      "...                 ...                                           ...\n",
      "5039  Silvio_Berlusconi  Silvio_Berlusconi/Silvio_Berlusconi_0022.jpg\n",
      "5039  Silvio_Berlusconi  Silvio_Berlusconi/Silvio_Berlusconi_0016.jpg\n",
      "5039  Silvio_Berlusconi  Silvio_Berlusconi/Silvio_Berlusconi_0014.jpg\n",
      "5039  Silvio_Berlusconi  Silvio_Berlusconi/Silvio_Berlusconi_0027.jpg\n",
      "5039  Silvio_Berlusconi  Silvio_Berlusconi/Silvio_Berlusconi_0028.jpg\n",
      "\n",
      "[750 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_for_each = 30\n",
    "tmp_l = []\n",
    "for name in list(image_paths['name'].value_counts()[:num_people].keys()):\n",
    "    tmp_l.append(image_paths[image_paths.name==name].sample(num_for_each))\n",
    "data = pd.concat(tmp_l)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "576ec0b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:53.712255Z",
     "iopub.status.busy": "2022-11-23T04:35:53.711583Z",
     "iopub.status.idle": "2022-11-23T04:35:53.717906Z",
     "shell.execute_reply": "2022-11-23T04:35:53.717054Z"
    },
    "papermill": {
     "duration": 0.014304,
     "end_time": "2022-11-23T04:35:53.720033",
     "exception": false,
     "start_time": "2022-11-23T04:35:53.705729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.2)\n",
    "data_train, data_val = train_test_split(data_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f32e31cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:53.731420Z",
     "iopub.status.busy": "2022-11-23T04:35:53.731155Z",
     "iopub.status.idle": "2022-11-23T04:35:53.735880Z",
     "shell.execute_reply": "2022-11-23T04:35:53.734980Z"
    },
    "papermill": {
     "duration": 0.013313,
     "end_time": "2022-11-23T04:35:53.738645",
     "exception": false,
     "start_time": "2022-11-23T04:35:53.725332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 2) (120, 2) (150, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape, data_val.shape, data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37a13536",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:53.751497Z",
     "iopub.status.busy": "2022-11-23T04:35:53.750652Z",
     "iopub.status.idle": "2022-11-23T04:35:59.343855Z",
     "shell.execute_reply": "2022-11-23T04:35:59.342853Z"
    },
    "papermill": {
     "duration": 5.601472,
     "end_time": "2022-11-23T04:35:59.346210",
     "exception": false,
     "start_time": "2022-11-23T04:35:53.744738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_root = './data/'\n",
    "\n",
    "data_list = [data_train, data_val, data_test]\n",
    "dirs = ['train', 'val', 'test']\n",
    "\n",
    "             # uncomment this line and run, to copy\n",
    "                #first remove data directory if it exists\n",
    "for i in range(len(dirs)):\n",
    "    pathlib.Path(os.path.join(data_root, dirs[i])).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for person in list_people:\n",
    "        if len(data_train[data_train['name']==person])>0:\n",
    "            pathlib.Path(os.path.join(data_root, dirs[i], person)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for im_path in data_list[i].image_path:\n",
    "        name = data[data['image_path']==im_path]['name'].iloc[0]\n",
    "        path_from = os.path.join(data_folder+'/lfw-deepfunneled/lfw-deepfunneled/', im_path)\n",
    "        path_to = os.path.join(data_root, dirs[i], name)\n",
    "        if not os.path.isfile(os.path.join(path_to, im_path)):\n",
    "            shutil.copy(path_from, path_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e34fee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:59.358365Z",
     "iopub.status.busy": "2022-11-23T04:35:59.358012Z",
     "iopub.status.idle": "2022-11-23T04:35:59.371435Z",
     "shell.execute_reply": "2022-11-23T04:35:59.370605Z"
    },
    "papermill": {
     "duration": 0.021442,
     "end_time": "2022-11-23T04:35:59.373281",
     "exception": false,
     "start_time": "2022-11-23T04:35:59.351839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = os.path.join(data_root, dirs[0])\n",
    "val_path = os.path.join(data_root, dirs[1])\n",
    "test_path = os.path.join(data_root, dirs[2])\n",
    "\n",
    "train_transform = transforms.Compose(transforms=[\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=0, std=255),      # output = (input-mean)/std\n",
    "])\n",
    "test_transform = transforms.Compose(transforms=[\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=0, std=255)\n",
    "])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path, train_transform), shuffle=True  ,batch_size = 10\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(val_path, test_transform), shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path, test_transform), shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ed8a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:59.386224Z",
     "iopub.status.busy": "2022-11-23T04:35:59.384698Z",
     "iopub.status.idle": "2022-11-23T04:35:59.438405Z",
     "shell.execute_reply": "2022-11-23T04:35:59.437430Z"
    },
    "papermill": {
     "duration": 0.062138,
     "end_time": "2022-11-23T04:35:59.440699",
     "exception": false,
     "start_time": "2022-11-23T04:35:59.378561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data[0].shape, data[1].shape)\n",
    "    break\n",
    "# Total train data is of shape (128, 3, 250, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4f56608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:59.453163Z",
     "iopub.status.busy": "2022-11-23T04:35:59.452874Z",
     "iopub.status.idle": "2022-11-23T04:35:59.465925Z",
     "shell.execute_reply": "2022-11-23T04:35:59.465025Z"
    },
    "papermill": {
     "duration": 0.021504,
     "end_time": "2022-11-23T04:35:59.467878",
     "exception": false,
     "start_time": "2022-11-23T04:35:59.446374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FaceCNN(nn.Module):\n",
    "    def __init__(self, num_classes, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.cnn_model = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        \n",
    "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        \n",
    "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=25088, out_features=4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=4096, out_features=4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=4096, out_features=1000),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=1000, out_features=num_classes),\n",
    "        nn.Softmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.cnn_model(input)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bd15ae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:35:59.479413Z",
     "iopub.status.busy": "2022-11-23T04:35:59.479143Z",
     "iopub.status.idle": "2022-11-23T04:36:03.942990Z",
     "shell.execute_reply": "2022-11-23T04:36:03.941684Z"
    },
    "papermill": {
     "duration": 4.472983,
     "end_time": "2022-11-23T04:36:03.945995",
     "exception": false,
     "start_time": "2022-11-23T04:35:59.473012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FaceCNN(num_classes=len(list_people))\n",
    "\"\"\"\"\n",
    "model =torchvision.models.vgg16(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "n_inputs = model.classifier[6].in_features\n",
    "\n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "    nn.Linear(n_inputs, 1000), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 200), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, len(list_people)), \n",
    "    nn.Softmax(dim=1))\n",
    "\"\"\"\n",
    "model = model.to(device)\n",
    "#model.classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d50576b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:36:03.963184Z",
     "iopub.status.busy": "2022-11-23T04:36:03.962838Z",
     "iopub.status.idle": "2022-11-23T04:36:03.969629Z",
     "shell.execute_reply": "2022-11-23T04:36:03.968328Z"
    },
    "papermill": {
     "duration": 0.019601,
     "end_time": "2022-11-23T04:36:03.972992",
     "exception": false,
     "start_time": "2022-11-23T04:36:03.953391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fba98db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:36:03.990709Z",
     "iopub.status.busy": "2022-11-23T04:36:03.990448Z",
     "iopub.status.idle": "2022-11-23T04:36:03.998865Z",
     "shell.execute_reply": "2022-11-23T04:36:03.997645Z"
    },
    "papermill": {
     "duration": 0.020029,
     "end_time": "2022-11-23T04:36:04.001517",
     "exception": false,
     "start_time": "2022-11-23T04:36:03.981488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    score = 0\n",
    "    cnt = 0\n",
    "\n",
    "    with torch.no_grad():       # not training, so no need to calculate gradients\n",
    "        for data in loader:\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(device)\n",
    "            # images, labels = data\n",
    "            output = model(data[0])\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            score += float(torch.sum(pred==data[1].data))\n",
    "            cnt += data[0].shape[0]\n",
    "\n",
    "    return score/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1553d089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:36:04.019484Z",
     "iopub.status.busy": "2022-11-23T04:36:04.019052Z",
     "iopub.status.idle": "2022-11-23T04:36:04.033794Z",
     "shell.execute_reply": "2022-11-23T04:36:04.033010Z"
    },
    "papermill": {
     "duration": 0.026323,
     "end_time": "2022-11-23T04:36:04.036205",
     "exception": false,
     "start_time": "2022-11-23T04:36:04.009882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "def train():\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_score = 0\n",
    "        cnt = 0\n",
    "        train_loss = 0\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            for i in range(len(batch)):\n",
    "                batch[i] = batch[i].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch[0])\n",
    "            label = batch[1]\n",
    "            \n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            train_score += float(torch.sum(pred==label.data))\n",
    "            cnt += batch[0].shape[0]\n",
    "\n",
    "        train_acc = train_score/cnt\n",
    "        val_acc = evaluate(val_loader, model)\n",
    "        train_accuracy.append(train_acc)\n",
    "        val_accuracy.append(val_acc)\n",
    "        \n",
    "        print(\"Epoch:\", epoch, \"\\tLoss:\", train_loss, \"\\tTraining Acc:\", train_acc, \"\\tVal Acc:\", val_acc)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            torch.save(model.state_dict(),'best.model')\n",
    "            best_acc = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cb0fe78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T04:36:04.052595Z",
     "iopub.status.busy": "2022-11-23T04:36:04.052301Z",
     "iopub.status.idle": "2022-11-23T05:35:55.565014Z",
     "shell.execute_reply": "2022-11-23T05:35:55.563227Z"
    },
    "papermill": {
     "duration": 3591.523939,
     "end_time": "2022-11-23T05:35:55.567895",
     "exception": false,
     "start_time": "2022-11-23T04:36:04.043956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 154.50610065460205 \tTraining Acc: 0.025 \tVal Acc: 0.016666666666666666\n",
      "Epoch: 1 \tLoss: 154.50380086898804 \tTraining Acc: 0.04375 \tVal Acc: 0.025\n",
      "Epoch: 2 \tLoss: 154.50337958335876 \tTraining Acc: 0.027083333333333334 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 3 \tLoss: 154.50204420089722 \tTraining Acc: 0.04375 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 4 \tLoss: 154.50192523002625 \tTraining Acc: 0.04375 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 5 \tLoss: 154.50068140029907 \tTraining Acc: 0.05625 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 6 \tLoss: 154.49947118759155 \tTraining Acc: 0.05 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 7 \tLoss: 154.49922442436218 \tTraining Acc: 0.05625 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 8 \tLoss: 154.49884343147278 \tTraining Acc: 0.041666666666666664 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 9 \tLoss: 154.49754810333252 \tTraining Acc: 0.05 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 10 \tLoss: 154.4960594177246 \tTraining Acc: 0.035416666666666666 \tVal Acc: 0.05\n",
      "Epoch: 11 \tLoss: 154.49354124069214 \tTraining Acc: 0.06041666666666667 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 12 \tLoss: 154.4936065673828 \tTraining Acc: 0.04791666666666667 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 13 \tLoss: 154.493497133255 \tTraining Acc: 0.05416666666666667 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 14 \tLoss: 154.49263668060303 \tTraining Acc: 0.05625 \tVal Acc: 0.05\n",
      "Epoch: 15 \tLoss: 154.49046325683594 \tTraining Acc: 0.06458333333333334 \tVal Acc: 0.05\n",
      "Epoch: 16 \tLoss: 154.4918713569641 \tTraining Acc: 0.05416666666666667 \tVal Acc: 0.05\n",
      "Epoch: 17 \tLoss: 154.48940205574036 \tTraining Acc: 0.06875 \tVal Acc: 0.05\n",
      "Epoch: 18 \tLoss: 154.48860669136047 \tTraining Acc: 0.06041666666666667 \tVal Acc: 0.05\n",
      "Epoch: 19 \tLoss: 154.48759746551514 \tTraining Acc: 0.06458333333333334 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 20 \tLoss: 154.48674845695496 \tTraining Acc: 0.05416666666666667 \tVal Acc: 0.05\n",
      "Epoch: 21 \tLoss: 154.4870584011078 \tTraining Acc: 0.052083333333333336 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 22 \tLoss: 154.48542308807373 \tTraining Acc: 0.07291666666666667 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 23 \tLoss: 154.48502111434937 \tTraining Acc: 0.08125 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 24 \tLoss: 154.4833860397339 \tTraining Acc: 0.075 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 25 \tLoss: 154.48306798934937 \tTraining Acc: 0.06041666666666667 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 26 \tLoss: 154.4825360774994 \tTraining Acc: 0.08125 \tVal Acc: 0.05\n",
      "Epoch: 27 \tLoss: 154.4815776348114 \tTraining Acc: 0.07083333333333333 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 28 \tLoss: 154.47954654693604 \tTraining Acc: 0.0875 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 29 \tLoss: 154.47813320159912 \tTraining Acc: 0.08958333333333333 \tVal Acc: 0.05\n",
      "Epoch: 30 \tLoss: 154.47841024398804 \tTraining Acc: 0.09166666666666666 \tVal Acc: 0.05\n",
      "Epoch: 31 \tLoss: 154.47764563560486 \tTraining Acc: 0.08541666666666667 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 32 \tLoss: 154.47650790214539 \tTraining Acc: 0.08541666666666667 \tVal Acc: 0.05\n",
      "Epoch: 33 \tLoss: 154.4771032333374 \tTraining Acc: 0.08541666666666667 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 34 \tLoss: 154.47455191612244 \tTraining Acc: 0.09166666666666666 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 35 \tLoss: 154.47583270072937 \tTraining Acc: 0.08958333333333333 \tVal Acc: 0.025\n",
      "Epoch: 36 \tLoss: 154.47306776046753 \tTraining Acc: 0.09166666666666666 \tVal Acc: 0.025\n",
      "Epoch: 37 \tLoss: 154.47275829315186 \tTraining Acc: 0.09166666666666666 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 38 \tLoss: 154.4730362892151 \tTraining Acc: 0.09583333333333334 \tVal Acc: 0.025\n",
      "Epoch: 39 \tLoss: 154.4708902835846 \tTraining Acc: 0.08333333333333333 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 40 \tLoss: 154.46983098983765 \tTraining Acc: 0.09791666666666667 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 41 \tLoss: 154.46988368034363 \tTraining Acc: 0.1 \tVal Acc: 0.025\n",
      "Epoch: 42 \tLoss: 154.46816897392273 \tTraining Acc: 0.1 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 43 \tLoss: 154.4680712223053 \tTraining Acc: 0.11041666666666666 \tVal Acc: 0.025\n",
      "Epoch: 44 \tLoss: 154.46618390083313 \tTraining Acc: 0.10833333333333334 \tVal Acc: 0.025\n",
      "Epoch: 45 \tLoss: 154.4672131538391 \tTraining Acc: 0.1125 \tVal Acc: 0.025\n",
      "Epoch: 46 \tLoss: 154.4650638103485 \tTraining Acc: 0.1125 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 47 \tLoss: 154.4640429019928 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 48 \tLoss: 154.46435928344727 \tTraining Acc: 0.10416666666666667 \tVal Acc: 0.025\n",
      "Epoch: 49 \tLoss: 154.46094822883606 \tTraining Acc: 0.10833333333333334 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 50 \tLoss: 154.46163845062256 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.025\n",
      "Epoch: 51 \tLoss: 154.4609715938568 \tTraining Acc: 0.12083333333333333 \tVal Acc: 0.025\n",
      "Epoch: 52 \tLoss: 154.45983791351318 \tTraining Acc: 0.12083333333333333 \tVal Acc: 0.025\n",
      "Epoch: 53 \tLoss: 154.46047520637512 \tTraining Acc: 0.12083333333333333 \tVal Acc: 0.025\n",
      "Epoch: 54 \tLoss: 154.45785284042358 \tTraining Acc: 0.13333333333333333 \tVal Acc: 0.025\n",
      "Epoch: 55 \tLoss: 154.458758354187 \tTraining Acc: 0.11875 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 56 \tLoss: 154.45662546157837 \tTraining Acc: 0.13125 \tVal Acc: 0.025\n",
      "Epoch: 57 \tLoss: 154.45522451400757 \tTraining Acc: 0.12291666666666666 \tVal Acc: 0.025\n",
      "Epoch: 58 \tLoss: 154.45526266098022 \tTraining Acc: 0.13958333333333334 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 59 \tLoss: 154.4534204006195 \tTraining Acc: 0.13541666666666666 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 60 \tLoss: 154.45378255844116 \tTraining Acc: 0.13333333333333333 \tVal Acc: 0.025\n",
      "Epoch: 61 \tLoss: 154.45255970954895 \tTraining Acc: 0.14583333333333334 \tVal Acc: 0.025\n",
      "Epoch: 62 \tLoss: 154.45186805725098 \tTraining Acc: 0.14583333333333334 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 63 \tLoss: 154.45122075080872 \tTraining Acc: 0.14583333333333334 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 64 \tLoss: 154.4508819580078 \tTraining Acc: 0.14583333333333334 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 65 \tLoss: 154.448805809021 \tTraining Acc: 0.16041666666666668 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 66 \tLoss: 154.4493579864502 \tTraining Acc: 0.14791666666666667 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 67 \tLoss: 154.44609212875366 \tTraining Acc: 0.1625 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 68 \tLoss: 154.44756054878235 \tTraining Acc: 0.14375 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 69 \tLoss: 154.44479060173035 \tTraining Acc: 0.16666666666666666 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 70 \tLoss: 154.4439959526062 \tTraining Acc: 0.14791666666666667 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 71 \tLoss: 154.44468784332275 \tTraining Acc: 0.17708333333333334 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 72 \tLoss: 154.44287753105164 \tTraining Acc: 0.15625 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 73 \tLoss: 154.44037008285522 \tTraining Acc: 0.16666666666666666 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 74 \tLoss: 154.4414632320404 \tTraining Acc: 0.15833333333333333 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 75 \tLoss: 154.44072675704956 \tTraining Acc: 0.16458333333333333 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 76 \tLoss: 154.43931031227112 \tTraining Acc: 0.1625 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 77 \tLoss: 154.4366476535797 \tTraining Acc: 0.17083333333333334 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 78 \tLoss: 154.43776059150696 \tTraining Acc: 0.14791666666666667 \tVal Acc: 0.03333333333333333\n",
      "Epoch: 79 \tLoss: 154.43675351142883 \tTraining Acc: 0.17291666666666666 \tVal Acc: 0.05\n",
      "Epoch: 80 \tLoss: 154.43673944473267 \tTraining Acc: 0.15625 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 81 \tLoss: 154.43648505210876 \tTraining Acc: 0.15416666666666667 \tVal Acc: 0.05\n",
      "Epoch: 82 \tLoss: 154.43406987190247 \tTraining Acc: 0.16041666666666668 \tVal Acc: 0.05\n",
      "Epoch: 83 \tLoss: 154.43248057365417 \tTraining Acc: 0.16458333333333333 \tVal Acc: 0.05\n",
      "Epoch: 84 \tLoss: 154.4336395263672 \tTraining Acc: 0.15 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 85 \tLoss: 154.43080186843872 \tTraining Acc: 0.17291666666666666 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 86 \tLoss: 154.43023920059204 \tTraining Acc: 0.18125 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 87 \tLoss: 154.42991137504578 \tTraining Acc: 0.15625 \tVal Acc: 0.075\n",
      "Epoch: 88 \tLoss: 154.42850255966187 \tTraining Acc: 0.15833333333333333 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 89 \tLoss: 154.42848682403564 \tTraining Acc: 0.16041666666666668 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 90 \tLoss: 154.4267327785492 \tTraining Acc: 0.1625 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 91 \tLoss: 154.42808628082275 \tTraining Acc: 0.15625 \tVal Acc: 0.075\n",
      "Epoch: 92 \tLoss: 154.4239411354065 \tTraining Acc: 0.18333333333333332 \tVal Acc: 0.075\n",
      "Epoch: 93 \tLoss: 154.42391443252563 \tTraining Acc: 0.17291666666666666 \tVal Acc: 0.075\n",
      "Epoch: 94 \tLoss: 154.422753572464 \tTraining Acc: 0.16875 \tVal Acc: 0.075\n",
      "Epoch: 95 \tLoss: 154.42227005958557 \tTraining Acc: 0.175 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 96 \tLoss: 154.4200210571289 \tTraining Acc: 0.17708333333333334 \tVal Acc: 0.075\n",
      "Epoch: 97 \tLoss: 154.4214825630188 \tTraining Acc: 0.17083333333333334 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 98 \tLoss: 154.4176368713379 \tTraining Acc: 0.175 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 99 \tLoss: 154.4186110496521 \tTraining Acc: 0.16875 \tVal Acc: 0.075\n",
      "Epoch: 100 \tLoss: 154.4170949459076 \tTraining Acc: 0.17083333333333334 \tVal Acc: 0.075\n",
      "Epoch: 101 \tLoss: 154.41557478904724 \tTraining Acc: 0.175 \tVal Acc: 0.075\n",
      "Epoch: 102 \tLoss: 154.4178569316864 \tTraining Acc: 0.17291666666666666 \tVal Acc: 0.075\n",
      "Epoch: 103 \tLoss: 154.41322875022888 \tTraining Acc: 0.17291666666666666 \tVal Acc: 0.075\n",
      "Epoch: 104 \tLoss: 154.41364860534668 \tTraining Acc: 0.17916666666666667 \tVal Acc: 0.075\n",
      "Epoch: 105 \tLoss: 154.41077184677124 \tTraining Acc: 0.17916666666666667 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 106 \tLoss: 154.4122133255005 \tTraining Acc: 0.18333333333333332 \tVal Acc: 0.075\n",
      "Epoch: 107 \tLoss: 154.4087851047516 \tTraining Acc: 0.1875 \tVal Acc: 0.075\n",
      "Epoch: 108 \tLoss: 154.40961503982544 \tTraining Acc: 0.17916666666666667 \tVal Acc: 0.075\n",
      "Epoch: 109 \tLoss: 154.40890502929688 \tTraining Acc: 0.17291666666666666 \tVal Acc: 0.075\n",
      "Epoch: 110 \tLoss: 154.40764379501343 \tTraining Acc: 0.17916666666666667 \tVal Acc: 0.075\n",
      "Epoch: 111 \tLoss: 154.40553045272827 \tTraining Acc: 0.19166666666666668 \tVal Acc: 0.075\n",
      "Epoch: 112 \tLoss: 154.40484952926636 \tTraining Acc: 0.18541666666666667 \tVal Acc: 0.075\n",
      "Epoch: 113 \tLoss: 154.40535807609558 \tTraining Acc: 0.18333333333333332 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 114 \tLoss: 154.40337705612183 \tTraining Acc: 0.175 \tVal Acc: 0.1\n",
      "Epoch: 115 \tLoss: 154.40087413787842 \tTraining Acc: 0.21875 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 116 \tLoss: 154.4009666442871 \tTraining Acc: 0.18333333333333332 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 117 \tLoss: 154.40077781677246 \tTraining Acc: 0.1875 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 118 \tLoss: 154.40088725090027 \tTraining Acc: 0.19375 \tVal Acc: 0.1\n",
      "Epoch: 119 \tLoss: 154.39633989334106 \tTraining Acc: 0.18125 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 120 \tLoss: 154.39609742164612 \tTraining Acc: 0.18333333333333332 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 121 \tLoss: 154.39444279670715 \tTraining Acc: 0.19375 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 122 \tLoss: 154.39260292053223 \tTraining Acc: 0.19375 \tVal Acc: 0.1\n",
      "Epoch: 123 \tLoss: 154.39100551605225 \tTraining Acc: 0.19375 \tVal Acc: 0.1\n",
      "Epoch: 124 \tLoss: 154.39298105239868 \tTraining Acc: 0.18333333333333332 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 125 \tLoss: 154.3900101184845 \tTraining Acc: 0.18125 \tVal Acc: 0.1\n",
      "Epoch: 126 \tLoss: 154.38883662223816 \tTraining Acc: 0.19791666666666666 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 127 \tLoss: 154.38834500312805 \tTraining Acc: 0.19791666666666666 \tVal Acc: 0.1\n",
      "Epoch: 128 \tLoss: 154.38817977905273 \tTraining Acc: 0.19375 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 129 \tLoss: 154.38534903526306 \tTraining Acc: 0.2 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 130 \tLoss: 154.384583234787 \tTraining Acc: 0.19166666666666668 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 131 \tLoss: 154.38683247566223 \tTraining Acc: 0.18541666666666667 \tVal Acc: 0.1\n",
      "Epoch: 132 \tLoss: 154.383198261261 \tTraining Acc: 0.19375 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 133 \tLoss: 154.38241839408875 \tTraining Acc: 0.18541666666666667 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 134 \tLoss: 154.38079047203064 \tTraining Acc: 0.20208333333333334 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 135 \tLoss: 154.38020777702332 \tTraining Acc: 0.20625 \tVal Acc: 0.1\n",
      "Epoch: 136 \tLoss: 154.37493014335632 \tTraining Acc: 0.20625 \tVal Acc: 0.1\n",
      "Epoch: 137 \tLoss: 154.37627267837524 \tTraining Acc: 0.19791666666666666 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 138 \tLoss: 154.375803232193 \tTraining Acc: 0.20833333333333334 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 139 \tLoss: 154.37364673614502 \tTraining Acc: 0.1875 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 140 \tLoss: 154.3699984550476 \tTraining Acc: 0.20416666666666666 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 141 \tLoss: 154.3699803352356 \tTraining Acc: 0.21041666666666667 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 142 \tLoss: 154.37175393104553 \tTraining Acc: 0.20625 \tVal Acc: 0.075\n",
      "Epoch: 143 \tLoss: 154.36577486991882 \tTraining Acc: 0.20416666666666666 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 144 \tLoss: 154.36810183525085 \tTraining Acc: 0.1875 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 145 \tLoss: 154.36647772789001 \tTraining Acc: 0.19583333333333333 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 146 \tLoss: 154.3643720149994 \tTraining Acc: 0.19583333333333333 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 147 \tLoss: 154.36259722709656 \tTraining Acc: 0.21041666666666667 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 148 \tLoss: 154.36156249046326 \tTraining Acc: 0.21041666666666667 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 149 \tLoss: 154.3584427833557 \tTraining Acc: 0.1875 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 150 \tLoss: 154.35866785049438 \tTraining Acc: 0.19166666666666668 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 151 \tLoss: 154.35986471176147 \tTraining Acc: 0.20833333333333334 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 152 \tLoss: 154.3594343662262 \tTraining Acc: 0.19375 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 153 \tLoss: 154.35364198684692 \tTraining Acc: 0.20208333333333334 \tVal Acc: 0.1\n",
      "Epoch: 154 \tLoss: 154.35351467132568 \tTraining Acc: 0.19583333333333333 \tVal Acc: 0.1\n",
      "Epoch: 155 \tLoss: 154.35415244102478 \tTraining Acc: 0.18958333333333333 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 156 \tLoss: 154.34832453727722 \tTraining Acc: 0.2 \tVal Acc: 0.1\n",
      "Epoch: 157 \tLoss: 154.34966135025024 \tTraining Acc: 0.19583333333333333 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 158 \tLoss: 154.3491599559784 \tTraining Acc: 0.21041666666666667 \tVal Acc: 0.1\n",
      "Epoch: 159 \tLoss: 154.34512567520142 \tTraining Acc: 0.20416666666666666 \tVal Acc: 0.1\n",
      "Epoch: 160 \tLoss: 154.34563899040222 \tTraining Acc: 0.20833333333333334 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 161 \tLoss: 154.34436225891113 \tTraining Acc: 0.20208333333333334 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 162 \tLoss: 154.33984923362732 \tTraining Acc: 0.18958333333333333 \tVal Acc: 0.1\n",
      "Epoch: 163 \tLoss: 154.34031558036804 \tTraining Acc: 0.20208333333333334 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 164 \tLoss: 154.33949899673462 \tTraining Acc: 0.19166666666666668 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 165 \tLoss: 154.33584356307983 \tTraining Acc: 0.2 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 166 \tLoss: 154.3344283103943 \tTraining Acc: 0.19583333333333333 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 167 \tLoss: 154.3340039253235 \tTraining Acc: 0.19583333333333333 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 168 \tLoss: 154.33195781707764 \tTraining Acc: 0.20416666666666666 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 169 \tLoss: 154.3314607143402 \tTraining Acc: 0.1875 \tVal Acc: 0.1\n",
      "Epoch: 170 \tLoss: 154.32910418510437 \tTraining Acc: 0.20416666666666666 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 171 \tLoss: 154.32792735099792 \tTraining Acc: 0.19791666666666666 \tVal Acc: 0.1\n",
      "Epoch: 172 \tLoss: 154.3257474899292 \tTraining Acc: 0.19791666666666666 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 173 \tLoss: 154.3255352973938 \tTraining Acc: 0.19583333333333333 \tVal Acc: 0.1\n",
      "Epoch: 174 \tLoss: 154.31934094429016 \tTraining Acc: 0.19583333333333333 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 175 \tLoss: 154.3175070285797 \tTraining Acc: 0.21458333333333332 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 176 \tLoss: 154.32038855552673 \tTraining Acc: 0.1875 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 177 \tLoss: 154.3155860900879 \tTraining Acc: 0.18333333333333332 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 178 \tLoss: 154.31669425964355 \tTraining Acc: 0.19375 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 179 \tLoss: 154.3154194355011 \tTraining Acc: 0.19583333333333333 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 180 \tLoss: 154.31339764595032 \tTraining Acc: 0.19583333333333333 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 181 \tLoss: 154.30718231201172 \tTraining Acc: 0.19791666666666666 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 182 \tLoss: 154.31054592132568 \tTraining Acc: 0.18333333333333332 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 183 \tLoss: 154.30730652809143 \tTraining Acc: 0.20208333333333334 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 184 \tLoss: 154.30350041389465 \tTraining Acc: 0.2 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 185 \tLoss: 154.30198454856873 \tTraining Acc: 0.19375 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 186 \tLoss: 154.3011817932129 \tTraining Acc: 0.19375 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 187 \tLoss: 154.2995662689209 \tTraining Acc: 0.17916666666666667 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 188 \tLoss: 154.2973654270172 \tTraining Acc: 0.19375 \tVal Acc: 0.11666666666666667\n",
      "Epoch: 189 \tLoss: 154.2951626777649 \tTraining Acc: 0.19583333333333333 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 190 \tLoss: 154.2939555644989 \tTraining Acc: 0.18958333333333333 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 191 \tLoss: 154.29010677337646 \tTraining Acc: 0.19583333333333333 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 192 \tLoss: 154.28677773475647 \tTraining Acc: 0.1875 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 193 \tLoss: 154.28801894187927 \tTraining Acc: 0.19166666666666668 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 194 \tLoss: 154.2821171283722 \tTraining Acc: 0.18333333333333332 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 195 \tLoss: 154.2833960056305 \tTraining Acc: 0.18958333333333333 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 196 \tLoss: 154.2826271057129 \tTraining Acc: 0.18541666666666667 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 197 \tLoss: 154.27882885932922 \tTraining Acc: 0.18333333333333332 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 198 \tLoss: 154.27741980552673 \tTraining Acc: 0.2 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 199 \tLoss: 154.2754933834076 \tTraining Acc: 0.175 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 200 \tLoss: 154.2679193019867 \tTraining Acc: 0.18541666666666667 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 201 \tLoss: 154.27013397216797 \tTraining Acc: 0.18125 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 202 \tLoss: 154.26439952850342 \tTraining Acc: 0.19375 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 203 \tLoss: 154.266663312912 \tTraining Acc: 0.17916666666666667 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 204 \tLoss: 154.26551580429077 \tTraining Acc: 0.18958333333333333 \tVal Acc: 0.1\n",
      "Epoch: 205 \tLoss: 154.26391458511353 \tTraining Acc: 0.17083333333333334 \tVal Acc: 0.1\n",
      "Epoch: 206 \tLoss: 154.25320982933044 \tTraining Acc: 0.17916666666666667 \tVal Acc: 0.1\n",
      "Epoch: 207 \tLoss: 154.25374937057495 \tTraining Acc: 0.18541666666666667 \tVal Acc: 0.1\n",
      "Epoch: 208 \tLoss: 154.2498037815094 \tTraining Acc: 0.175 \tVal Acc: 0.1\n",
      "Epoch: 209 \tLoss: 154.24869632720947 \tTraining Acc: 0.18333333333333332 \tVal Acc: 0.1\n",
      "Epoch: 210 \tLoss: 154.25147891044617 \tTraining Acc: 0.17916666666666667 \tVal Acc: 0.1\n",
      "Epoch: 211 \tLoss: 154.24167585372925 \tTraining Acc: 0.17708333333333334 \tVal Acc: 0.1\n",
      "Epoch: 212 \tLoss: 154.24074578285217 \tTraining Acc: 0.16875 \tVal Acc: 0.1\n",
      "Epoch: 213 \tLoss: 154.24077820777893 \tTraining Acc: 0.18541666666666667 \tVal Acc: 0.1\n",
      "Epoch: 214 \tLoss: 154.24137425422668 \tTraining Acc: 0.17291666666666666 \tVal Acc: 0.1\n",
      "Epoch: 215 \tLoss: 154.2322280406952 \tTraining Acc: 0.17291666666666666 \tVal Acc: 0.1\n",
      "Epoch: 216 \tLoss: 154.2324080467224 \tTraining Acc: 0.175 \tVal Acc: 0.1\n",
      "Epoch: 217 \tLoss: 154.2242021560669 \tTraining Acc: 0.17291666666666666 \tVal Acc: 0.1\n",
      "Epoch: 218 \tLoss: 154.22435307502747 \tTraining Acc: 0.16875 \tVal Acc: 0.1\n",
      "Epoch: 219 \tLoss: 154.220144033432 \tTraining Acc: 0.17291666666666666 \tVal Acc: 0.1\n",
      "Epoch: 220 \tLoss: 154.2180027961731 \tTraining Acc: 0.15833333333333333 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 221 \tLoss: 154.2163007259369 \tTraining Acc: 0.17708333333333334 \tVal Acc: 0.1\n",
      "Epoch: 222 \tLoss: 154.21355032920837 \tTraining Acc: 0.15625 \tVal Acc: 0.1\n",
      "Epoch: 223 \tLoss: 154.20921659469604 \tTraining Acc: 0.175 \tVal Acc: 0.1\n",
      "Epoch: 224 \tLoss: 154.20416498184204 \tTraining Acc: 0.16666666666666666 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 225 \tLoss: 154.20032691955566 \tTraining Acc: 0.16875 \tVal Acc: 0.1\n",
      "Epoch: 226 \tLoss: 154.19796538352966 \tTraining Acc: 0.16041666666666668 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 227 \tLoss: 154.19707918167114 \tTraining Acc: 0.17083333333333334 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 228 \tLoss: 154.19432878494263 \tTraining Acc: 0.15625 \tVal Acc: 0.1\n",
      "Epoch: 229 \tLoss: 154.18569564819336 \tTraining Acc: 0.16875 \tVal Acc: 0.1\n",
      "Epoch: 230 \tLoss: 154.17958641052246 \tTraining Acc: 0.15625 \tVal Acc: 0.1\n",
      "Epoch: 231 \tLoss: 154.17947006225586 \tTraining Acc: 0.1625 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 232 \tLoss: 154.17790293693542 \tTraining Acc: 0.15625 \tVal Acc: 0.1\n",
      "Epoch: 233 \tLoss: 154.17549443244934 \tTraining Acc: 0.1625 \tVal Acc: 0.1\n",
      "Epoch: 234 \tLoss: 154.1665301322937 \tTraining Acc: 0.1625 \tVal Acc: 0.1\n",
      "Epoch: 235 \tLoss: 154.1674098968506 \tTraining Acc: 0.15208333333333332 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 236 \tLoss: 154.16392374038696 \tTraining Acc: 0.15 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 237 \tLoss: 154.15852117538452 \tTraining Acc: 0.14791666666666667 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 238 \tLoss: 154.15117263793945 \tTraining Acc: 0.1625 \tVal Acc: 0.1\n",
      "Epoch: 239 \tLoss: 154.1369924545288 \tTraining Acc: 0.15416666666666667 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 240 \tLoss: 154.144278049469 \tTraining Acc: 0.14791666666666667 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 241 \tLoss: 154.13258242607117 \tTraining Acc: 0.16041666666666668 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 242 \tLoss: 154.13450741767883 \tTraining Acc: 0.14375 \tVal Acc: 0.1\n",
      "Epoch: 243 \tLoss: 154.1339316368103 \tTraining Acc: 0.14375 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 244 \tLoss: 154.11919379234314 \tTraining Acc: 0.16458333333333333 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 245 \tLoss: 154.12135004997253 \tTraining Acc: 0.1375 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 246 \tLoss: 154.11734700202942 \tTraining Acc: 0.14166666666666666 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 247 \tLoss: 154.10643243789673 \tTraining Acc: 0.15 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 248 \tLoss: 154.09732747077942 \tTraining Acc: 0.15208333333333332 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 249 \tLoss: 154.0987093448639 \tTraining Acc: 0.13541666666666666 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 250 \tLoss: 154.0955193042755 \tTraining Acc: 0.125 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 251 \tLoss: 154.07891178131104 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 252 \tLoss: 154.07466506958008 \tTraining Acc: 0.1375 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 253 \tLoss: 154.0790412425995 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 254 \tLoss: 154.043762922287 \tTraining Acc: 0.125 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 255 \tLoss: 154.05264830589294 \tTraining Acc: 0.12291666666666666 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 256 \tLoss: 154.038476228714 \tTraining Acc: 0.12083333333333333 \tVal Acc: 0.075\n",
      "Epoch: 257 \tLoss: 154.0328130722046 \tTraining Acc: 0.11666666666666667 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 258 \tLoss: 154.02884602546692 \tTraining Acc: 0.13541666666666666 \tVal Acc: 0.075\n",
      "Epoch: 259 \tLoss: 154.0186107158661 \tTraining Acc: 0.11666666666666667 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 260 \tLoss: 153.9959614276886 \tTraining Acc: 0.1125 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 261 \tLoss: 153.9935929775238 \tTraining Acc: 0.11458333333333333 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 262 \tLoss: 153.96501755714417 \tTraining Acc: 0.12083333333333333 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 263 \tLoss: 153.95169138908386 \tTraining Acc: 0.10208333333333333 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 264 \tLoss: 153.93625020980835 \tTraining Acc: 0.10625 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 265 \tLoss: 153.94340586662292 \tTraining Acc: 0.09791666666666667 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 266 \tLoss: 153.89500737190247 \tTraining Acc: 0.1 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 267 \tLoss: 153.89874076843262 \tTraining Acc: 0.1 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 268 \tLoss: 153.8680775165558 \tTraining Acc: 0.08333333333333333 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 269 \tLoss: 153.87635731697083 \tTraining Acc: 0.075 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 270 \tLoss: 153.83941960334778 \tTraining Acc: 0.07916666666666666 \tVal Acc: 0.05\n",
      "Epoch: 271 \tLoss: 153.82942867279053 \tTraining Acc: 0.07291666666666667 \tVal Acc: 0.05\n",
      "Epoch: 272 \tLoss: 153.79230427742004 \tTraining Acc: 0.07708333333333334 \tVal Acc: 0.05\n",
      "Epoch: 273 \tLoss: 153.74972558021545 \tTraining Acc: 0.06458333333333334 \tVal Acc: 0.05\n",
      "Epoch: 274 \tLoss: 153.73601293563843 \tTraining Acc: 0.05625 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 275 \tLoss: 153.69519901275635 \tTraining Acc: 0.058333333333333334 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 276 \tLoss: 153.67927193641663 \tTraining Acc: 0.058333333333333334 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 277 \tLoss: 153.63607144355774 \tTraining Acc: 0.05416666666666667 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 278 \tLoss: 153.62049293518066 \tTraining Acc: 0.05625 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 279 \tLoss: 153.56047368049622 \tTraining Acc: 0.05 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 280 \tLoss: 153.50417113304138 \tTraining Acc: 0.04791666666666667 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 281 \tLoss: 153.43616819381714 \tTraining Acc: 0.05 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 282 \tLoss: 153.42844128608704 \tTraining Acc: 0.052083333333333336 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 283 \tLoss: 153.3735237121582 \tTraining Acc: 0.05 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 284 \tLoss: 153.32709550857544 \tTraining Acc: 0.04791666666666667 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 285 \tLoss: 153.29440784454346 \tTraining Acc: 0.052083333333333336 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 286 \tLoss: 153.26566219329834 \tTraining Acc: 0.06041666666666667 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 287 \tLoss: 153.23140168190002 \tTraining Acc: 0.05625 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 288 \tLoss: 153.19568061828613 \tTraining Acc: 0.06458333333333334 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 289 \tLoss: 153.19600296020508 \tTraining Acc: 0.06666666666666667 \tVal Acc: 0.041666666666666664\n",
      "Epoch: 290 \tLoss: 153.1569230556488 \tTraining Acc: 0.075 \tVal Acc: 0.05\n",
      "Epoch: 291 \tLoss: 153.13374400138855 \tTraining Acc: 0.07083333333333333 \tVal Acc: 0.05\n",
      "Epoch: 292 \tLoss: 153.05305409431458 \tTraining Acc: 0.08125 \tVal Acc: 0.05\n",
      "Epoch: 293 \tLoss: 153.08359599113464 \tTraining Acc: 0.07708333333333334 \tVal Acc: 0.05\n",
      "Epoch: 294 \tLoss: 153.07425689697266 \tTraining Acc: 0.08333333333333333 \tVal Acc: 0.05\n",
      "Epoch: 295 \tLoss: 153.03426218032837 \tTraining Acc: 0.08958333333333333 \tVal Acc: 0.05\n",
      "Epoch: 296 \tLoss: 153.00553727149963 \tTraining Acc: 0.09166666666666666 \tVal Acc: 0.05\n",
      "Epoch: 297 \tLoss: 152.99925303459167 \tTraining Acc: 0.08958333333333333 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 298 \tLoss: 152.95308446884155 \tTraining Acc: 0.09791666666666667 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 299 \tLoss: 152.98029828071594 \tTraining Acc: 0.10416666666666667 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 300 \tLoss: 152.92958569526672 \tTraining Acc: 0.10208333333333333 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 301 \tLoss: 152.9147334098816 \tTraining Acc: 0.10833333333333334 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 302 \tLoss: 152.88241314888 \tTraining Acc: 0.1125 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 303 \tLoss: 152.89131259918213 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 304 \tLoss: 152.87107753753662 \tTraining Acc: 0.125 \tVal Acc: 0.075\n",
      "Epoch: 305 \tLoss: 152.7838454246521 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.075\n",
      "Epoch: 306 \tLoss: 152.8833680152893 \tTraining Acc: 0.12083333333333333 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 307 \tLoss: 152.77946424484253 \tTraining Acc: 0.11875 \tVal Acc: 0.075\n",
      "Epoch: 308 \tLoss: 152.76216340065002 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.075\n",
      "Epoch: 309 \tLoss: 152.75124263763428 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.075\n",
      "Epoch: 310 \tLoss: 152.77388906478882 \tTraining Acc: 0.13541666666666666 \tVal Acc: 0.075\n",
      "Epoch: 311 \tLoss: 152.79144382476807 \tTraining Acc: 0.1375 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 312 \tLoss: 152.7720127105713 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 313 \tLoss: 152.72339153289795 \tTraining Acc: 0.13125 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 314 \tLoss: 152.7030544281006 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 315 \tLoss: 152.6902163028717 \tTraining Acc: 0.13125 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 316 \tLoss: 152.67692470550537 \tTraining Acc: 0.13541666666666666 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 317 \tLoss: 152.64118576049805 \tTraining Acc: 0.1375 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 318 \tLoss: 152.64389371871948 \tTraining Acc: 0.1375 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 319 \tLoss: 152.6197566986084 \tTraining Acc: 0.15 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 320 \tLoss: 152.6491243839264 \tTraining Acc: 0.14375 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 321 \tLoss: 152.61614775657654 \tTraining Acc: 0.15625 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 322 \tLoss: 152.63563776016235 \tTraining Acc: 0.14166666666666666 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 323 \tLoss: 152.63815665245056 \tTraining Acc: 0.15208333333333332 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 324 \tLoss: 152.59478068351746 \tTraining Acc: 0.13958333333333334 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 325 \tLoss: 152.55850982666016 \tTraining Acc: 0.13958333333333334 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 326 \tLoss: 152.53283977508545 \tTraining Acc: 0.14166666666666666 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 327 \tLoss: 152.53385162353516 \tTraining Acc: 0.15208333333333332 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 328 \tLoss: 152.5371241569519 \tTraining Acc: 0.14166666666666666 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 329 \tLoss: 152.52387523651123 \tTraining Acc: 0.15 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 330 \tLoss: 152.5021402835846 \tTraining Acc: 0.14791666666666667 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 331 \tLoss: 152.52547359466553 \tTraining Acc: 0.15208333333333332 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 332 \tLoss: 152.52385711669922 \tTraining Acc: 0.15208333333333332 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 333 \tLoss: 152.43950152397156 \tTraining Acc: 0.15 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 334 \tLoss: 152.4624845981598 \tTraining Acc: 0.14791666666666667 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 335 \tLoss: 152.42392325401306 \tTraining Acc: 0.14583333333333334 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 336 \tLoss: 152.4617886543274 \tTraining Acc: 0.13958333333333334 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 337 \tLoss: 152.4110083580017 \tTraining Acc: 0.15 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 338 \tLoss: 152.44952249526978 \tTraining Acc: 0.14375 \tVal Acc: 0.075\n",
      "Epoch: 339 \tLoss: 152.3647699356079 \tTraining Acc: 0.14791666666666667 \tVal Acc: 0.075\n",
      "Epoch: 340 \tLoss: 152.37145328521729 \tTraining Acc: 0.14166666666666666 \tVal Acc: 0.075\n",
      "Epoch: 341 \tLoss: 152.38111519813538 \tTraining Acc: 0.14166666666666666 \tVal Acc: 0.075\n",
      "Epoch: 342 \tLoss: 152.34845566749573 \tTraining Acc: 0.13958333333333334 \tVal Acc: 0.075\n",
      "Epoch: 343 \tLoss: 152.3084053993225 \tTraining Acc: 0.14791666666666667 \tVal Acc: 0.075\n",
      "Epoch: 344 \tLoss: 152.34346914291382 \tTraining Acc: 0.13958333333333334 \tVal Acc: 0.075\n",
      "Epoch: 345 \tLoss: 152.30622267723083 \tTraining Acc: 0.13333333333333333 \tVal Acc: 0.075\n",
      "Epoch: 346 \tLoss: 152.25796055793762 \tTraining Acc: 0.14375 \tVal Acc: 0.075\n",
      "Epoch: 347 \tLoss: 152.31243443489075 \tTraining Acc: 0.1375 \tVal Acc: 0.075\n",
      "Epoch: 348 \tLoss: 152.2850580215454 \tTraining Acc: 0.13125 \tVal Acc: 0.075\n",
      "Epoch: 349 \tLoss: 152.21149826049805 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 350 \tLoss: 152.21416640281677 \tTraining Acc: 0.13125 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 351 \tLoss: 152.21030688285828 \tTraining Acc: 0.125 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 352 \tLoss: 152.17892146110535 \tTraining Acc: 0.12083333333333333 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 353 \tLoss: 152.18548774719238 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 354 \tLoss: 152.15852046012878 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 355 \tLoss: 152.10784125328064 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 356 \tLoss: 152.13048815727234 \tTraining Acc: 0.125 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 357 \tLoss: 152.10362696647644 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 358 \tLoss: 152.07807755470276 \tTraining Acc: 0.125 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 359 \tLoss: 152.00822854042053 \tTraining Acc: 0.12291666666666666 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 360 \tLoss: 151.98636269569397 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 361 \tLoss: 152.04507160186768 \tTraining Acc: 0.125 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 362 \tLoss: 152.00730729103088 \tTraining Acc: 0.12291666666666666 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 363 \tLoss: 151.96013593673706 \tTraining Acc: 0.12291666666666666 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 364 \tLoss: 151.9237825870514 \tTraining Acc: 0.125 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 365 \tLoss: 151.93577027320862 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 366 \tLoss: 151.85040521621704 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 367 \tLoss: 151.80184364318848 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.075\n",
      "Epoch: 368 \tLoss: 151.76867246627808 \tTraining Acc: 0.13125 \tVal Acc: 0.08333333333333333\n",
      "Epoch: 369 \tLoss: 151.7386474609375 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 370 \tLoss: 151.65665769577026 \tTraining Acc: 0.13333333333333333 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 371 \tLoss: 151.63165020942688 \tTraining Acc: 0.13125 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 372 \tLoss: 151.6313693523407 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 373 \tLoss: 151.55318808555603 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 374 \tLoss: 151.48288822174072 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 375 \tLoss: 151.38654398918152 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 376 \tLoss: 151.30955410003662 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 377 \tLoss: 151.25217008590698 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 378 \tLoss: 151.15696454048157 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 379 \tLoss: 151.10126519203186 \tTraining Acc: 0.125 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 380 \tLoss: 150.96773290634155 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 381 \tLoss: 150.89049577713013 \tTraining Acc: 0.13125 \tVal Acc: 0.058333333333333334\n",
      "Epoch: 382 \tLoss: 150.7963490486145 \tTraining Acc: 0.12708333333333333 \tVal Acc: 0.075\n",
      "Epoch: 383 \tLoss: 150.71469712257385 \tTraining Acc: 0.13125 \tVal Acc: 0.075\n",
      "Epoch: 384 \tLoss: 150.59409642219543 \tTraining Acc: 0.12916666666666668 \tVal Acc: 0.075\n",
      "Epoch: 385 \tLoss: 150.51156282424927 \tTraining Acc: 0.13125 \tVal Acc: 0.06666666666666667\n",
      "Epoch: 386 \tLoss: 150.41732597351074 \tTraining Acc: 0.13541666666666666 \tVal Acc: 0.075\n",
      "Epoch: 387 \tLoss: 150.45641827583313 \tTraining Acc: 0.1375 \tVal Acc: 0.075\n",
      "Epoch: 388 \tLoss: 150.31548953056335 \tTraining Acc: 0.1375 \tVal Acc: 0.075\n",
      "Epoch: 389 \tLoss: 150.23858308792114 \tTraining Acc: 0.15833333333333333 \tVal Acc: 0.075\n",
      "Epoch: 390 \tLoss: 150.1969292163849 \tTraining Acc: 0.15416666666666667 \tVal Acc: 0.075\n",
      "Epoch: 391 \tLoss: 150.07437539100647 \tTraining Acc: 0.15625 \tVal Acc: 0.075\n",
      "Epoch: 392 \tLoss: 150.03127264976501 \tTraining Acc: 0.16041666666666668 \tVal Acc: 0.1\n",
      "Epoch: 393 \tLoss: 149.9655797481537 \tTraining Acc: 0.16458333333333333 \tVal Acc: 0.1\n",
      "Epoch: 394 \tLoss: 149.95552372932434 \tTraining Acc: 0.16458333333333333 \tVal Acc: 0.09166666666666666\n",
      "Epoch: 395 \tLoss: 149.80406951904297 \tTraining Acc: 0.16875 \tVal Acc: 0.1\n",
      "Epoch: 396 \tLoss: 149.76785397529602 \tTraining Acc: 0.16458333333333333 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 397 \tLoss: 149.74617171287537 \tTraining Acc: 0.16458333333333333 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 398 \tLoss: 149.59287905693054 \tTraining Acc: 0.1625 \tVal Acc: 0.1\n",
      "Epoch: 399 \tLoss: 149.59430766105652 \tTraining Acc: 0.17291666666666666 \tVal Acc: 0.1\n",
      "Epoch: 400 \tLoss: 149.5195336341858 \tTraining Acc: 0.16875 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 401 \tLoss: 149.37290573120117 \tTraining Acc: 0.16875 \tVal Acc: 0.1\n",
      "Epoch: 402 \tLoss: 149.43099403381348 \tTraining Acc: 0.17083333333333334 \tVal Acc: 0.1\n",
      "Epoch: 403 \tLoss: 149.3560025691986 \tTraining Acc: 0.17916666666666667 \tVal Acc: 0.1\n",
      "Epoch: 404 \tLoss: 149.30269289016724 \tTraining Acc: 0.17083333333333334 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 405 \tLoss: 149.31917810440063 \tTraining Acc: 0.17083333333333334 \tVal Acc: 0.1\n",
      "Epoch: 406 \tLoss: 149.22363448143005 \tTraining Acc: 0.18125 \tVal Acc: 0.125\n",
      "Epoch: 407 \tLoss: 149.1265299320221 \tTraining Acc: 0.17708333333333334 \tVal Acc: 0.1\n",
      "Epoch: 408 \tLoss: 149.15491724014282 \tTraining Acc: 0.17916666666666667 \tVal Acc: 0.11666666666666667\n",
      "Epoch: 409 \tLoss: 149.02863597869873 \tTraining Acc: 0.17916666666666667 \tVal Acc: 0.125\n",
      "Epoch: 410 \tLoss: 149.0312123298645 \tTraining Acc: 0.18541666666666667 \tVal Acc: 0.11666666666666667\n",
      "Epoch: 411 \tLoss: 148.88633918762207 \tTraining Acc: 0.18333333333333332 \tVal Acc: 0.13333333333333333\n",
      "Epoch: 412 \tLoss: 148.79394364356995 \tTraining Acc: 0.19166666666666668 \tVal Acc: 0.13333333333333333\n",
      "Epoch: 413 \tLoss: 148.8298795223236 \tTraining Acc: 0.19166666666666668 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 414 \tLoss: 148.75242567062378 \tTraining Acc: 0.19166666666666668 \tVal Acc: 0.15\n",
      "Epoch: 415 \tLoss: 148.7007439136505 \tTraining Acc: 0.1875 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 416 \tLoss: 148.5546350479126 \tTraining Acc: 0.19791666666666666 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 417 \tLoss: 148.43375825881958 \tTraining Acc: 0.2 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 418 \tLoss: 148.45488739013672 \tTraining Acc: 0.20416666666666666 \tVal Acc: 0.15\n",
      "Epoch: 419 \tLoss: 148.33947849273682 \tTraining Acc: 0.20625 \tVal Acc: 0.125\n",
      "Epoch: 420 \tLoss: 148.28789854049683 \tTraining Acc: 0.20625 \tVal Acc: 0.125\n",
      "Epoch: 421 \tLoss: 148.1984624862671 \tTraining Acc: 0.20625 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 422 \tLoss: 148.17715311050415 \tTraining Acc: 0.21458333333333332 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 423 \tLoss: 147.94987082481384 \tTraining Acc: 0.2125 \tVal Acc: 0.15\n",
      "Epoch: 424 \tLoss: 148.07551336288452 \tTraining Acc: 0.21041666666666667 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 425 \tLoss: 148.0324831008911 \tTraining Acc: 0.2125 \tVal Acc: 0.13333333333333333\n",
      "Epoch: 426 \tLoss: 147.77963614463806 \tTraining Acc: 0.2125 \tVal Acc: 0.13333333333333333\n",
      "Epoch: 427 \tLoss: 147.85279750823975 \tTraining Acc: 0.2125 \tVal Acc: 0.15\n",
      "Epoch: 428 \tLoss: 147.73838758468628 \tTraining Acc: 0.21666666666666667 \tVal Acc: 0.13333333333333333\n",
      "Epoch: 429 \tLoss: 147.6127336025238 \tTraining Acc: 0.2125 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 430 \tLoss: 147.66156315803528 \tTraining Acc: 0.2125 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 431 \tLoss: 147.57672119140625 \tTraining Acc: 0.21458333333333332 \tVal Acc: 0.125\n",
      "Epoch: 432 \tLoss: 147.46335530281067 \tTraining Acc: 0.21458333333333332 \tVal Acc: 0.13333333333333333\n",
      "Epoch: 433 \tLoss: 147.415935754776 \tTraining Acc: 0.21666666666666667 \tVal Acc: 0.13333333333333333\n",
      "Epoch: 434 \tLoss: 147.36533641815186 \tTraining Acc: 0.225 \tVal Acc: 0.10833333333333334\n",
      "Epoch: 435 \tLoss: 147.3935468196869 \tTraining Acc: 0.22083333333333333 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 436 \tLoss: 147.1809675693512 \tTraining Acc: 0.22083333333333333 \tVal Acc: 0.15\n",
      "Epoch: 437 \tLoss: 147.08884596824646 \tTraining Acc: 0.22916666666666666 \tVal Acc: 0.15\n",
      "Epoch: 438 \tLoss: 146.98343801498413 \tTraining Acc: 0.23333333333333334 \tVal Acc: 0.15\n",
      "Epoch: 439 \tLoss: 147.08723640441895 \tTraining Acc: 0.22291666666666668 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 440 \tLoss: 147.0126187801361 \tTraining Acc: 0.23333333333333334 \tVal Acc: 0.15833333333333333\n",
      "Epoch: 441 \tLoss: 146.8346929550171 \tTraining Acc: 0.2375 \tVal Acc: 0.15833333333333333\n",
      "Epoch: 442 \tLoss: 146.85864663124084 \tTraining Acc: 0.2375 \tVal Acc: 0.13333333333333333\n",
      "Epoch: 443 \tLoss: 146.77223134040833 \tTraining Acc: 0.23541666666666666 \tVal Acc: 0.125\n",
      "Epoch: 444 \tLoss: 146.7146031856537 \tTraining Acc: 0.24375 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 445 \tLoss: 146.52123546600342 \tTraining Acc: 0.24791666666666667 \tVal Acc: 0.15833333333333333\n",
      "Epoch: 446 \tLoss: 146.554869890213 \tTraining Acc: 0.2375 \tVal Acc: 0.15\n",
      "Epoch: 447 \tLoss: 146.49891638755798 \tTraining Acc: 0.25 \tVal Acc: 0.15\n",
      "Epoch: 448 \tLoss: 146.59309911727905 \tTraining Acc: 0.24791666666666667 \tVal Acc: 0.15\n",
      "Epoch: 449 \tLoss: 146.479412317276 \tTraining Acc: 0.24375 \tVal Acc: 0.15\n",
      "Epoch: 450 \tLoss: 146.25291991233826 \tTraining Acc: 0.25 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 451 \tLoss: 146.35624265670776 \tTraining Acc: 0.24166666666666667 \tVal Acc: 0.15\n",
      "Epoch: 452 \tLoss: 146.25143337249756 \tTraining Acc: 0.2520833333333333 \tVal Acc: 0.175\n",
      "Epoch: 453 \tLoss: 146.1756453514099 \tTraining Acc: 0.25 \tVal Acc: 0.15833333333333333\n",
      "Epoch: 454 \tLoss: 146.1527123451233 \tTraining Acc: 0.25416666666666665 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 455 \tLoss: 145.97201991081238 \tTraining Acc: 0.2520833333333333 \tVal Acc: 0.18333333333333332\n",
      "Epoch: 456 \tLoss: 145.9535207748413 \tTraining Acc: 0.25416666666666665 \tVal Acc: 0.16666666666666666\n",
      "Epoch: 457 \tLoss: 145.77645230293274 \tTraining Acc: 0.2520833333333333 \tVal Acc: 0.16666666666666666\n",
      "Epoch: 458 \tLoss: 145.74123072624207 \tTraining Acc: 0.2520833333333333 \tVal Acc: 0.15833333333333333\n",
      "Epoch: 459 \tLoss: 145.68588209152222 \tTraining Acc: 0.25625 \tVal Acc: 0.15833333333333333\n",
      "Epoch: 460 \tLoss: 145.62417602539062 \tTraining Acc: 0.25833333333333336 \tVal Acc: 0.15833333333333333\n",
      "Epoch: 461 \tLoss: 145.57240986824036 \tTraining Acc: 0.25625 \tVal Acc: 0.15\n",
      "Epoch: 462 \tLoss: 145.48915600776672 \tTraining Acc: 0.2625 \tVal Acc: 0.15\n",
      "Epoch: 463 \tLoss: 145.50185441970825 \tTraining Acc: 0.2604166666666667 \tVal Acc: 0.125\n",
      "Epoch: 464 \tLoss: 145.31818270683289 \tTraining Acc: 0.2604166666666667 \tVal Acc: 0.15833333333333333\n",
      "Epoch: 465 \tLoss: 145.25325441360474 \tTraining Acc: 0.26666666666666666 \tVal Acc: 0.18333333333333332\n",
      "Epoch: 466 \tLoss: 145.24814367294312 \tTraining Acc: 0.27291666666666664 \tVal Acc: 0.16666666666666666\n",
      "Epoch: 467 \tLoss: 145.08885049819946 \tTraining Acc: 0.27708333333333335 \tVal Acc: 0.18333333333333332\n",
      "Epoch: 468 \tLoss: 145.0439794063568 \tTraining Acc: 0.2875 \tVal Acc: 0.2\n",
      "Epoch: 469 \tLoss: 145.04209518432617 \tTraining Acc: 0.2791666666666667 \tVal Acc: 0.18333333333333332\n",
      "Epoch: 470 \tLoss: 144.88352966308594 \tTraining Acc: 0.29375 \tVal Acc: 0.2\n",
      "Epoch: 471 \tLoss: 145.0264880657196 \tTraining Acc: 0.28541666666666665 \tVal Acc: 0.18333333333333332\n",
      "Epoch: 472 \tLoss: 144.75865864753723 \tTraining Acc: 0.29375 \tVal Acc: 0.18333333333333332\n",
      "Epoch: 473 \tLoss: 144.8678262233734 \tTraining Acc: 0.28958333333333336 \tVal Acc: 0.15833333333333333\n",
      "Epoch: 474 \tLoss: 144.6808044910431 \tTraining Acc: 0.29375 \tVal Acc: 0.19166666666666668\n",
      "Epoch: 475 \tLoss: 144.54061555862427 \tTraining Acc: 0.29583333333333334 \tVal Acc: 0.20833333333333334\n",
      "Epoch: 476 \tLoss: 144.5976402759552 \tTraining Acc: 0.29791666666666666 \tVal Acc: 0.2\n",
      "Epoch: 477 \tLoss: 144.42610788345337 \tTraining Acc: 0.3 \tVal Acc: 0.19166666666666668\n",
      "Epoch: 478 \tLoss: 144.37519884109497 \tTraining Acc: 0.30833333333333335 \tVal Acc: 0.15\n",
      "Epoch: 479 \tLoss: 144.28479075431824 \tTraining Acc: 0.3020833333333333 \tVal Acc: 0.15\n",
      "Epoch: 480 \tLoss: 144.2083797454834 \tTraining Acc: 0.30416666666666664 \tVal Acc: 0.20833333333333334\n",
      "Epoch: 481 \tLoss: 144.33879590034485 \tTraining Acc: 0.30625 \tVal Acc: 0.18333333333333332\n",
      "Epoch: 482 \tLoss: 144.0162627696991 \tTraining Acc: 0.3145833333333333 \tVal Acc: 0.2\n",
      "Epoch: 483 \tLoss: 143.95213389396667 \tTraining Acc: 0.31666666666666665 \tVal Acc: 0.14166666666666666\n",
      "Epoch: 484 \tLoss: 144.00313186645508 \tTraining Acc: 0.32708333333333334 \tVal Acc: 0.19166666666666668\n",
      "Epoch: 485 \tLoss: 143.90840697288513 \tTraining Acc: 0.32083333333333336 \tVal Acc: 0.2\n",
      "Epoch: 486 \tLoss: 143.8906648159027 \tTraining Acc: 0.32708333333333334 \tVal Acc: 0.18333333333333332\n",
      "Epoch: 487 \tLoss: 143.8802330493927 \tTraining Acc: 0.32916666666666666 \tVal Acc: 0.20833333333333334\n",
      "Epoch: 488 \tLoss: 143.86234283447266 \tTraining Acc: 0.3229166666666667 \tVal Acc: 0.13333333333333333\n",
      "Epoch: 489 \tLoss: 143.65199208259583 \tTraining Acc: 0.3333333333333333 \tVal Acc: 0.18333333333333332\n",
      "Epoch: 490 \tLoss: 143.49225854873657 \tTraining Acc: 0.33958333333333335 \tVal Acc: 0.20833333333333334\n",
      "Epoch: 491 \tLoss: 143.69626593589783 \tTraining Acc: 0.3333333333333333 \tVal Acc: 0.21666666666666667\n",
      "Epoch: 492 \tLoss: 143.36860966682434 \tTraining Acc: 0.33541666666666664 \tVal Acc: 0.19166666666666668\n",
      "Epoch: 493 \tLoss: 143.31627774238586 \tTraining Acc: 0.34375 \tVal Acc: 0.18333333333333332\n",
      "Epoch: 494 \tLoss: 143.31784629821777 \tTraining Acc: 0.32916666666666666 \tVal Acc: 0.2\n",
      "Epoch: 495 \tLoss: 142.97882986068726 \tTraining Acc: 0.33958333333333335 \tVal Acc: 0.13333333333333333\n",
      "Epoch: 496 \tLoss: 143.0665807723999 \tTraining Acc: 0.33541666666666664 \tVal Acc: 0.2\n",
      "Epoch: 497 \tLoss: 142.9654667377472 \tTraining Acc: 0.33958333333333335 \tVal Acc: 0.11666666666666667\n",
      "Epoch: 498 \tLoss: 142.98457789421082 \tTraining Acc: 0.34375 \tVal Acc: 0.19166666666666668\n",
      "Epoch: 499 \tLoss: 142.87695217132568 \tTraining Acc: 0.34375 \tVal Acc: 0.20833333333333334\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb57b005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T05:35:55.628592Z",
     "iopub.status.busy": "2022-11-23T05:35:55.628307Z",
     "iopub.status.idle": "2022-11-23T05:35:55.861292Z",
     "shell.execute_reply": "2022-11-23T05:35:55.860392Z"
    },
    "papermill": {
     "duration": 0.265295,
     "end_time": "2022-11-23T05:35:55.863283",
     "exception": false,
     "start_time": "2022-11-23T05:35:55.597988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch = []\n",
    "for i in range(num_epochs):\n",
    "    epoch.append(i+1)\n",
    "plt.plot(epoch,train_accuracy)\n",
    "plt.plot(epoch,val_accuracy)\n",
    "plt.savefig(\"VGG16-SGD-Batchnorm.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79510132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T05:35:55.924413Z",
     "iopub.status.busy": "2022-11-23T05:35:55.923619Z",
     "iopub.status.idle": "2022-11-23T05:35:55.931975Z",
     "shell.execute_reply": "2022-11-23T05:35:55.930703Z"
    },
    "papermill": {
     "duration": 0.04117,
     "end_time": "2022-11-23T05:35:55.934523",
     "exception": false,
     "start_time": "2022-11-23T05:35:55.893353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "with open(\"train.txt\",'w',encoding = 'utf-8') as f:\n",
    "   for i in train_accuracy:\n",
    "    f.write(str(i)+\"\\n\")\n",
    "    \n",
    "with open(\"val.txt\",'w',encoding = 'utf-8') as f:\n",
    "   for i in val_accuracy:\n",
    "    f.write(str(i)+\"\\n\")\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05611d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T05:35:55.995941Z",
     "iopub.status.busy": "2022-11-23T05:35:55.995171Z",
     "iopub.status.idle": "2022-11-23T05:35:56.560188Z",
     "shell.execute_reply": "2022-11-23T05:35:56.558840Z"
    },
    "papermill": {
     "duration": 0.597174,
     "end_time": "2022-11-23T05:35:56.561778",
     "exception": true,
     "start_time": "2022-11-23T05:35:55.964604",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23/4134554476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_model\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "best_model = torch.load('best.model')\n",
    "best_model  = best_model.to(device)\n",
    "score = 0\n",
    "cnt = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        # images, labels = data\n",
    "        output = best_model(data[0])\n",
    "        _, pred = torch.max(output.data, 1)\n",
    "        score += float(torch.sum(pred==data[1].data))\n",
    "        cnt += data[0].shape[0]\n",
    "\n",
    "print(score/cnt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3616.827283,
   "end_time": "2022-11-23T05:35:58.762024",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-23T04:35:41.934741",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
