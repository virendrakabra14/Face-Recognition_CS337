{"cells":[{"cell_type":"code","execution_count":161,"metadata":{"execution":{"iopub.execute_input":"2022-11-23T23:47:42.899311Z","iopub.status.busy":"2022-11-23T23:47:42.898732Z","iopub.status.idle":"2022-11-23T23:47:55.781524Z","shell.execute_reply":"2022-11-23T23:47:55.780188Z","shell.execute_reply.started":"2022-11-23T23:47:42.899264Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","try:\n","    from torchinfo import summary\n","except ModuleNotFoundError:\n","    !pip install torchinfo\n","    from torchinfo import summary\n","\n","import os\n","import pathlib\n","import shutil\n","import sys"]},{"cell_type":"code","execution_count":162,"metadata":{"execution":{"iopub.execute_input":"2022-11-23T23:47:55.784754Z","iopub.status.busy":"2022-11-23T23:47:55.784301Z","iopub.status.idle":"2022-11-23T23:47:55.794266Z","shell.execute_reply":"2022-11-23T23:47:55.793180Z","shell.execute_reply.started":"2022-11-23T23:47:55.784705Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\nSame dir structure as on Kaggle\\ninput/\\n    lfw-dataset/\\n        csv files\\n        lfw-deepfunneled/\\nworking/\\n    notebook\\n    data/\\n        train/\\n        val/\\n        test/\\n'"]},"execution_count":162,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","Same dir structure as on Kaggle\n","input/\n","    lfw-dataset/\n","        csv files\n","        lfw-deepfunneled/\n","working/\n","    notebook\n","    data/\n","        train/\n","        val/\n","        test/\n","\"\"\""]},{"cell_type":"code","execution_count":163,"metadata":{"execution":{"iopub.execute_input":"2022-11-23T23:47:55.796471Z","iopub.status.busy":"2022-11-23T23:47:55.796028Z","iopub.status.idle":"2022-11-23T23:47:55.939037Z","shell.execute_reply":"2022-11-23T23:47:55.938164Z","shell.execute_reply.started":"2022-11-23T23:47:55.796433Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]},{"data":{"text/plain":["<torch._C.Generator at 0x271b4961770>"]},"execution_count":163,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)\n","\n","# https://towardsdatascience.com/pytorch-switching-to-the-gpu-a7c0b21e8a99\n","# for modifications to use GPU\n","\n","# Also this: https://github.com/pytorch/examples/blob/main/imagenet/main.py\n","\n","np.random.seed(0)\n","torch.random.manual_seed(0)"]},{"cell_type":"code","execution_count":164,"metadata":{"execution":{"iopub.execute_input":"2022-11-23T23:47:55.942154Z","iopub.status.busy":"2022-11-23T23:47:55.941762Z","iopub.status.idle":"2022-11-23T23:47:55.948401Z","shell.execute_reply":"2022-11-23T23:47:55.947428Z","shell.execute_reply.started":"2022-11-23T23:47:55.942100Z"},"trusted":true},"outputs":[],"source":["data_folder = '../input/lfw-dataset/'"]},{"cell_type":"code","execution_count":165,"metadata":{"execution":{"iopub.execute_input":"2022-11-23T23:47:55.951543Z","iopub.status.busy":"2022-11-23T23:47:55.951045Z","iopub.status.idle":"2022-11-23T23:47:56.028133Z","shell.execute_reply":"2022-11-23T23:47:56.027163Z","shell.execute_reply.started":"2022-11-23T23:47:55.951517Z"},"trusted":true},"outputs":[],"source":["lfw_allnames = pd.read_csv(data_folder+\"lfw_allnames.csv\")\n","\n","image_paths = lfw_allnames.loc[lfw_allnames.index.repeat(lfw_allnames['images'])]\n","image_paths['image_path'] = 1 + image_paths.groupby('name').cumcount()\n","image_paths['image_path'] = image_paths.image_path.apply(lambda x: str(x).zfill(4))\n","image_paths['image_path'] = image_paths.name + \"/\" + image_paths.name + \"_\" + image_paths.image_path + \".jpg\"\n","image_paths = image_paths.drop(\"images\", axis=1)"]},{"cell_type":"code","execution_count":166,"metadata":{"execution":{"iopub.execute_input":"2022-11-23T23:47:56.029705Z","iopub.status.busy":"2022-11-23T23:47:56.029361Z","iopub.status.idle":"2022-11-23T23:47:56.049686Z","shell.execute_reply":"2022-11-23T23:47:56.048791Z","shell.execute_reply.started":"2022-11-23T23:47:56.029666Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["George_W_Bush        530\n","Colin_Powell         236\n","Tony_Blair           144\n","Donald_Rumsfeld      121\n","Gerhard_Schroeder    109\n","Ariel_Sharon          77\n","Hugo_Chavez           71\n","Junichiro_Koizumi     60\n","Jean_Chretien         55\n","John_Ashcroft         53\n","Name: name, dtype: int64\n","['George_W_Bush', 'Colin_Powell', 'Tony_Blair', 'Donald_Rumsfeld', 'Gerhard_Schroeder', 'Ariel_Sharon', 'Hugo_Chavez', 'Junichiro_Koizumi', 'Jean_Chretien', 'John_Ashcroft'] [530, 236, 144, 121, 109, 77, 71, 60, 55, 53]\n"]}],"source":["num_ppl = 10\n","\n","print(image_paths['name'].value_counts()[:num_ppl])\n","list_people = list(image_paths['name'].value_counts()[:num_ppl].keys())\n","list_num_images = list(image_paths['name'].value_counts()[:num_ppl])\n","print(list_people, list_num_images)"]},{"cell_type":"code","execution_count":167,"metadata":{"execution":{"iopub.execute_input":"2022-11-23T23:47:56.051600Z","iopub.status.busy":"2022-11-23T23:47:56.051233Z","iopub.status.idle":"2022-11-23T23:47:56.057996Z","shell.execute_reply":"2022-11-23T23:47:56.056753Z","shell.execute_reply.started":"2022-11-23T23:47:56.051548Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"\\nnum_for_each = image_paths['name'].value_counts()[num_ppl-1]\\ntmp_l = []\\nfor name in list(image_paths['name'].value_counts()[:num_ppl].keys()):\\n    tmp_l.append(image_paths[image_paths.name==name].sample(num_for_each))\\ndata = pd.concat(tmp_l)\\nprint(data)\\n\""]},"execution_count":167,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","num_for_each = image_paths['name'].value_counts()[num_ppl-1]\n","tmp_l = []\n","for name in list(image_paths['name'].value_counts()[:num_ppl].keys()):\n","    tmp_l.append(image_paths[image_paths.name==name].sample(num_for_each))\n","data = pd.concat(tmp_l)\n","print(data)\n","\"\"\""]},{"cell_type":"code","execution_count":168,"metadata":{"execution":{"iopub.execute_input":"2022-11-23T23:47:56.060269Z","iopub.status.busy":"2022-11-23T23:47:56.059897Z","iopub.status.idle":"2022-11-23T23:47:56.100820Z","shell.execute_reply":"2022-11-23T23:47:56.099833Z","shell.execute_reply.started":"2022-11-23T23:47:56.060236Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(330, 2) (90, 2) (110, 2)\n"]}],"source":["num_for_each = image_paths['name'].value_counts()[num_ppl-1]\n","tmp_train = []\n","tmp_val = []\n","tmp_test = []\n","for name in list(image_paths['name'].value_counts()[:num_ppl].keys()):\n","    data_all = image_paths[image_paths.name==name].sample(num_for_each)\n","    data_train, data_test = train_test_split(data_all, test_size=0.2)\n","    data_train, data_val = train_test_split(data_train, test_size=0.2)\n","    tmp_train.append(data_train.copy())\n","    tmp_val.append(data_val.copy())\n","    tmp_test.append(data_test.copy())\n","data_train = pd.concat(tmp_train)\n","data_val = pd.concat(tmp_val)\n","data_test = pd.concat(tmp_test)\n","print(data_train.shape, data_val.shape, data_test.shape)\n"]},{"cell_type":"code","execution_count":169,"metadata":{"execution":{"iopub.execute_input":"2022-11-23T23:47:56.102586Z","iopub.status.busy":"2022-11-23T23:47:56.102247Z","iopub.status.idle":"2022-11-23T23:48:03.465045Z","shell.execute_reply":"2022-11-23T23:48:03.462855Z","shell.execute_reply.started":"2022-11-23T23:47:56.102553Z"},"trusted":true},"outputs":[],"source":["data_root = './data/'\n","\n","data_list = [data_train, data_val, data_test]\n","dirs = ['train', 'val', 'test']\n","\n","# \"\"\"             # (un)comment this line (only) and run, to copy\n","\n","# # remove data directory if it exists\n","if os.path.exists(data_root) and os.path.isdir(data_root):\n","    shutil.rmtree(data_root)\n","\n","transform_augment = transforms.Compose([\n","    transforms.RandomHorizontalFlip(p=1)\n","])\n","\n","for i in range(len(dirs)):\n","    pathlib.Path(os.path.join(data_root, dirs[i])).mkdir(parents=True, exist_ok=True)\n","    \n","    data = data_list[i]\n","\n","    for person in list_people:\n","        if len(data_train[data_train['name']==person])>0:\n","            pathlib.Path(os.path.join(data_root, dirs[i], person)).mkdir(parents=True, exist_ok=True)\n","\n","    for im_path in data_list[i].image_path:\n","        name = data[data['image_path']==im_path]['name'].iloc[0]\n","        path_from = os.path.join(data_folder+'/lfw-deepfunneled/', im_path)\n","        filename, file_extension = os.path.splitext(path_from.split('/')[-1])\n","        path_to = os.path.join(data_root, dirs[i], name)\n","\n","        if not os.path.isfile(os.path.join(path_to, im_path)):\n","            shutil.copy(path_from, path_to)         # earlier (just copies image)\n","            \n","            # if dirs[i]!='test':                   # test-time augmentation too?\n","            img = Image.open(path_from)\n","            img = transform_augment(img)            # transformed image\n","            img.save(path_to+'/'+filename+'_transformed'+file_extension)\n","\n","# \"\"\""]},{"cell_type":"code","execution_count":170,"metadata":{"execution":{"iopub.execute_input":"2022-11-24T00:16:48.658057Z","iopub.status.busy":"2022-11-24T00:16:48.657001Z","iopub.status.idle":"2022-11-24T00:16:48.677947Z","shell.execute_reply":"2022-11-24T00:16:48.676794Z","shell.execute_reply.started":"2022-11-24T00:16:48.658015Z"},"trusted":true},"outputs":[],"source":["train_path = os.path.join(data_root, dirs[0])\n","val_path = os.path.join(data_root, dirs[1])\n","test_path = os.path.join(data_root, dirs[2])\n","\n","train_transform = transforms.Compose(transforms=[\n","    # transforms.RandomHorizontalFlip(),\n","    # transforms.Grayscale(num_output_channels=1),         # convert to grayscale\n","    transforms.ToTensor(),\n","    # transforms.Normalize(mean=0, std=255),      # output = (input-mean)/std\n","])\n","test_transform = transforms.Compose(transforms=[\n","    # transforms.Grayscale(num_output_channels=1),         # convert to grayscale\n","    transforms.ToTensor(),\n","    # transforms.Normalize(mean=0, std=255)\n","])\n","\n","dataloader_kwargs = {\n","    'pin_memory': True,\n","    'num_workers': 1,\n","    'batch_size': 1,\n","    'shuffle': True\n","}\n","dataloader_kwargs_triplet = {\n","    'pin_memory': True,\n","    'num_workers': 1,\n","    'batch_size': 4,\n","    'shuffle': True\n","}\n","non_blocking = dataloader_kwargs['pin_memory']  # https://stackoverflow.com/questions/55563376/\n","\n","train_loader = DataLoader(\n","    torchvision.datasets.ImageFolder(train_path, train_transform), **dataloader_kwargs\n",")\n","train_loader_triplet = DataLoader(\n","    torchvision.datasets.ImageFolder(train_path, train_transform), **dataloader_kwargs_triplet\n",")\n","val_loader = DataLoader(\n","    torchvision.datasets.ImageFolder(val_path, test_transform), **dataloader_kwargs\n",")\n","test_loader = DataLoader(\n","    torchvision.datasets.ImageFolder(test_path, test_transform), **dataloader_kwargs\n",")"]},{"cell_type":"code","execution_count":171,"metadata":{"execution":{"iopub.execute_input":"2022-11-24T00:16:49.048226Z","iopub.status.busy":"2022-11-24T00:16:49.047846Z","iopub.status.idle":"2022-11-24T00:16:49.155632Z","shell.execute_reply":"2022-11-24T00:16:49.154429Z","shell.execute_reply.started":"2022-11-24T00:16:49.048194Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 3, 250, 250]) torch.Size([1])\n","tensor(0.3739)\n"]}],"source":["for data in train_loader:\n","    print(data[0].shape, data[1].shape)\n","    # print(data[0], data[1])\n","    print(torch.mean(data[0]))\n","    break\n","# Total train data is of shape (128, 3, 250, 250)"]},{"cell_type":"code","execution_count":172,"metadata":{"execution":{"iopub.execute_input":"2022-11-24T00:16:49.309085Z","iopub.status.busy":"2022-11-24T00:16:49.308026Z","iopub.status.idle":"2022-11-24T00:16:49.317453Z","shell.execute_reply":"2022-11-24T00:16:49.316185Z","shell.execute_reply.started":"2022-11-24T00:16:49.309045Z"},"trusted":true},"outputs":[],"source":["class FaceCNN_initial(nn.Module):\n","    def __init__(self, num_input_channels, num_classes, stride=1, padding=1):\n","        super().__init__()\n","\n","        self.network = nn.Sequential(\n","\n","        nn.Conv2d(in_channels=num_input_channels, out_channels=50, kernel_size=3, stride=stride, padding=padding),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2),\n","\n","        nn.Conv2d(in_channels=50, out_channels=20, kernel_size=3, stride=stride, padding=padding),\n","        nn.ReLU(),\n","\n","        nn.Flatten(),\n","        nn.Linear(in_features=20*125*125, out_features=num_classes)\n","\n","        )\n","\n","    def forward(self, input):\n","        output = self.network(input)\n","        return output"]},{"cell_type":"code","execution_count":173,"metadata":{"execution":{"iopub.execute_input":"2022-11-24T00:16:49.633095Z","iopub.status.busy":"2022-11-24T00:16:49.632662Z","iopub.status.idle":"2022-11-24T00:16:49.652650Z","shell.execute_reply":"2022-11-24T00:16:49.651478Z","shell.execute_reply.started":"2022-11-24T00:16:49.633057Z"},"trusted":true},"outputs":[],"source":["class FaceCNN(nn.Module):\n","    def __init__(self, num_input_channels, num_classes, stride=1, padding=1):\n","        super().__init__()\n","\n","        self.encoder = nn.Sequential(\n","\n","            # (250, 250, 3)\n","\n","            nn.Conv2d(in_channels=num_input_channels, out_channels=64, kernel_size=7, stride=2, padding=padding),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Dropout(p=0.2),\n","\n","            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=padding),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Dropout(p=0.2),\n","\n","            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=stride, padding=padding),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Dropout(p=0.2),\n","\n","            nn.Conv2d(in_channels=256, out_channels=64, kernel_size=3, stride=stride, padding=padding),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            # nn.MaxPool2d(kernel_size=2),\n","            nn.Dropout(p=0.2),\n","\n","            nn.Flatten(),\n","            nn.Linear(in_features=14400, out_features=1024),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.5),    # https://stats.stackexchange.com/questions/240305/\n","            nn.Linear(in_features=1024, out_features=64),\n","            nn.ReLU(),\n","        )\n","        self.decoder = nn.Sequential(        \n","            nn.Dropout(p=0.5),\n","            nn.Linear(in_features=64, out_features=num_classes),\n","        )\n","\n","    def forward(self, input):\n","        encoded = self.encoder(input)\n","        output = self.decoder(encoded)\n","        return output, encoded"]},{"cell_type":"code","execution_count":174,"metadata":{"execution":{"iopub.execute_input":"2022-11-24T00:33:15.208657Z","iopub.status.busy":"2022-11-24T00:33:15.208275Z","iopub.status.idle":"2022-11-24T00:33:15.218687Z","shell.execute_reply":"2022-11-24T00:33:15.217611Z","shell.execute_reply.started":"2022-11-24T00:33:15.208624Z"},"trusted":true},"outputs":[],"source":["def triplet_loss_fn(f, Y, num_classes, alpha=1, lam=0.01):\n","    # f is num_samples x output_dim_of_encoder (=64)\n","    # Y is categorical of size num_classes\n","    loss = 0\n","    for c in range(num_classes):\n","        p = f[Y==c]\n","        n = f[~(Y==c)]\n","        p_self = torch.sum((p[:,None,:]-p[None,:,:])**2, dim=2)\n","        p_n = torch.sum((p[:,None,:]-n[None,:,:])**2, dim=2)\n","        loss += torch.sum(torch.relu(p_self[:,:,None]-p_n[:,None,:]+alpha))\n","    return loss*lam"]},{"cell_type":"code","execution_count":175,"metadata":{"execution":{"iopub.execute_input":"2022-11-24T00:38:14.221909Z","iopub.status.busy":"2022-11-24T00:38:14.221528Z","iopub.status.idle":"2022-11-24T00:38:14.403383Z","shell.execute_reply":"2022-11-24T00:38:14.402287Z","shell.execute_reply.started":"2022-11-24T00:38:14.221876Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","FaceCNN                                  [1, 10]                   --\n","├─Sequential: 1-1                        [1, 64]                   --\n","│    └─Conv2d: 2-1                       [1, 64, 123, 123]         9,472\n","│    └─BatchNorm2d: 2-2                  [1, 64, 123, 123]         128\n","│    └─ReLU: 2-3                         [1, 64, 123, 123]         --\n","│    └─MaxPool2d: 2-4                    [1, 64, 61, 61]           --\n","│    └─Dropout: 2-5                      [1, 64, 61, 61]           --\n","│    └─Conv2d: 2-6                       [1, 128, 61, 61]          73,856\n","│    └─BatchNorm2d: 2-7                  [1, 128, 61, 61]          256\n","│    └─ReLU: 2-8                         [1, 128, 61, 61]          --\n","│    └─MaxPool2d: 2-9                    [1, 128, 30, 30]          --\n","│    └─Dropout: 2-10                     [1, 128, 30, 30]          --\n","│    └─Conv2d: 2-11                      [1, 256, 30, 30]          295,168\n","│    └─BatchNorm2d: 2-12                 [1, 256, 30, 30]          512\n","│    └─ReLU: 2-13                        [1, 256, 30, 30]          --\n","│    └─MaxPool2d: 2-14                   [1, 256, 15, 15]          --\n","│    └─Dropout: 2-15                     [1, 256, 15, 15]          --\n","│    └─Conv2d: 2-16                      [1, 64, 15, 15]           147,520\n","│    └─BatchNorm2d: 2-17                 [1, 64, 15, 15]           128\n","│    └─ReLU: 2-18                        [1, 64, 15, 15]           --\n","│    └─Dropout: 2-19                     [1, 64, 15, 15]           --\n","│    └─Flatten: 2-20                     [1, 14400]                --\n","│    └─Linear: 2-21                      [1, 1024]                 14,746,624\n","│    └─ReLU: 2-22                        [1, 1024]                 --\n","│    └─Dropout: 2-23                     [1, 1024]                 --\n","│    └─Linear: 2-24                      [1, 64]                   65,600\n","│    └─ReLU: 2-25                        [1, 64]                   --\n","├─Sequential: 1-2                        [1, 10]                   --\n","│    └─Dropout: 2-26                     [1, 64]                   --\n","│    └─Linear: 2-27                      [1, 10]                   650\n","==========================================================================================\n","Total params: 15,339,914\n","Trainable params: 15,339,914\n","Non-trainable params: 0\n","Total mult-adds (M): 731.78\n","==========================================================================================\n","Input size (MB): 0.75\n","Forward/backward pass size (MB): 27.04\n","Params size (MB): 61.36\n","Estimated Total Size (MB): 89.15\n","==========================================================================================\n"]}],"source":["num_input_channels = 3\n","model = FaceCNN(num_input_channels=num_input_channels, num_classes=len(list_people)).to(device)\n","print(summary(model, input_size=(dataloader_kwargs['batch_size'], num_input_channels, 250, 250)))\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3)\n","loss_fn = nn.CrossEntropyLoss()\n","num_epochs = 30"]},{"cell_type":"code","execution_count":176,"metadata":{"execution":{"iopub.execute_input":"2022-11-24T00:38:14.409820Z","iopub.status.busy":"2022-11-24T00:38:14.408970Z","iopub.status.idle":"2022-11-24T00:38:14.416473Z","shell.execute_reply":"2022-11-24T00:38:14.415512Z","shell.execute_reply.started":"2022-11-24T00:38:14.409780Z"},"trusted":true},"outputs":[],"source":["def evaluate(loader, model):\n","\n","    model.eval()\n","\n","    score = 0\n","    cnt = 0\n","\n","    with torch.no_grad():       # not training, so no need to calculate gradients\n","        for inputs, labels in loader:\n","            inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","            output, _ = model(inputs)\n","            _, pred = torch.max(output.data, 1)\n","            score += float(torch.sum(pred==labels.data))\n","            cnt += data[0].shape[0]\n","\n","    return score/cnt"]},{"cell_type":"code","execution_count":177,"metadata":{"execution":{"iopub.execute_input":"2022-11-24T00:38:14.585625Z","iopub.status.busy":"2022-11-24T00:38:14.585266Z","iopub.status.idle":"2022-11-24T00:38:14.597415Z","shell.execute_reply":"2022-11-24T00:38:14.596333Z","shell.execute_reply.started":"2022-11-24T00:38:14.585595Z"},"trusted":true},"outputs":[],"source":["def train():\n","    best_acc = 0.0\n","    \n","    for epoch in range(num_epochs):\n","        train_score = 0\n","        cnt = 0\n","        CE_loss = 0\n","        triplet_loss = 0\n","\n","        model.train()\n","        \n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","\n","            optimizer.zero_grad()\n","            \n","            outputs,_ = model(inputs)\n","            \n","            # print(outputs, labels)\n","\n","            loss = loss_fn(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            CE_loss += loss.item()\n","\n","            _, preds = torch.max(outputs.data, 1)\n","            train_score += float(torch.sum(preds==labels.data))\n","            cnt += inputs.shape[0]\n","\n","            # print(preds, labels)\n","\n","        for inputs, labels in train_loader_triplet:\n","            inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","            optimizer.zero_grad()\n","            outputs, encoded = model(inputs)\n","            loss = triplet_loss_fn(encoded, labels, len(list_people))\n","            loss.backward()\n","            optimizer.step()\n","            \n","            triplet_loss += loss.item()\n","\n","        train_acc = train_score/cnt\n","        val_acc = evaluate(val_loader, model)\n","        \n","        print(\"Epoch:\", epoch, \"\\tLoss:\", CE_loss, triplet_loss, \"\\tTraining Acc:\", train_acc, \"\\tVal Acc:\", val_acc)\n","\n","        if val_acc > best_acc:\n","            torch.save(model.state_dict(),'best.model')\n","            best_acc = val_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-24T00:38:14.870253Z","iopub.status.busy":"2022-11-24T00:38:14.869854Z","iopub.status.idle":"2022-11-24T00:44:19.660727Z","shell.execute_reply":"2022-11-24T00:44:19.659374Z","shell.execute_reply.started":"2022-11-24T00:38:14.870221Z"},"trusted":true},"outputs":[],"source":["train()"]},{"cell_type":"code","execution_count":179,"metadata":{"execution":{"iopub.execute_input":"2022-11-24T00:44:19.666441Z","iopub.status.busy":"2022-11-24T00:44:19.663824Z","iopub.status.idle":"2022-11-24T00:44:20.968900Z","shell.execute_reply":"2022-11-24T00:44:20.967800Z","shell.execute_reply.started":"2022-11-24T00:44:19.666397Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7727272727272727\n"]}],"source":["model.load_state_dict(torch.load('best.model'))\n","model.eval()\n","score = 0\n","cnt = 0\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","        output, encoded = model(inputs)\n","        _, pred = torch.max(output.data, 1)\n","        score += float(torch.sum(pred==labels.data))\n","        cnt += data[0].shape[0]\n","\n","print(score/cnt)"]},{"cell_type":"code","execution_count":180,"metadata":{},"outputs":[],"source":["model.load_state_dict(torch.load('best.model'))\n","model.eval()\n","\n","arr_encoder = torch.zeros((2*data_test.shape[0],64))\n","arr_labels = torch.zeros((2*data_test.shape[0]))\n","idx = 0\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","        output, encoded = model(inputs)\n","        arr_encoder[idx] = encoded\n","        arr_labels[idx] = labels\n","        idx += 1"]},{"cell_type":"code","execution_count":181,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["class : 0 closeness ratio : 0.19873519241809845\n","class : 1 closeness ratio : 0.4159530699253082\n","class : 2 closeness ratio : 0.2434469610452652\n","class : 3 closeness ratio : 0.30418264865875244\n","class : 4 closeness ratio : 0.28869783878326416\n","class : 5 closeness ratio : 0.2094588577747345\n","class : 6 closeness ratio : 0.2653326690196991\n","class : 7 closeness ratio : 0.32717618346214294\n","class : 8 closeness ratio : 0.17419537901878357\n","class : 9 closeness ratio : 0.38288360834121704\n"]}],"source":["for c in range(num_ppl):\n","    p = arr_encoder[arr_labels==c]\n","    n = arr_encoder[~(arr_labels==c)]\n","    p_self = torch.mean(torch.sum((p[:,None,:]-p[None,:,:])**2, dim=2))\n","    p_n = torch.mean(torch.sum((p[:,None,:]-n[None,:,:])**2, dim=2))\n","    print(f\"class : {c} closeness ratio : {p_self/p_n}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Adding extra Faces\n","\n","Suppose now, we need to add some extra (say 5) faces to the model. We won't need to retrain the whole model again. We just need to retrain the final layer, which acts as a decoder, while the rest of the system is an encoder for the features and remains the same."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["George_W_Bush                530\n","Colin_Powell                 236\n","Tony_Blair                   144\n","Donald_Rumsfeld              121\n","Gerhard_Schroeder            109\n","Ariel_Sharon                  77\n","Hugo_Chavez                   71\n","Junichiro_Koizumi             60\n","Jean_Chretien                 55\n","John_Ashcroft                 53\n","Jacques_Chirac                52\n","Serena_Williams               52\n","Vladimir_Putin                49\n","Luiz_Inacio_Lula_da_Silva     48\n","Gloria_Macapagal_Arroyo       44\n","Name: name, dtype: int64\n","['George_W_Bush', 'Colin_Powell', 'Tony_Blair', 'Donald_Rumsfeld', 'Gerhard_Schroeder', 'Ariel_Sharon', 'Hugo_Chavez', 'Junichiro_Koizumi', 'Jean_Chretien', 'John_Ashcroft', 'Jacques_Chirac', 'Serena_Williams', 'Vladimir_Putin', 'Luiz_Inacio_Lula_da_Silva', 'Gloria_Macapagal_Arroyo'] [530, 236, 144, 121, 109, 77, 71, 60, 55, 53, 52, 52, 49, 48, 44]\n","(420, 2) (105, 2) (135, 2)\n"]}],"source":["# num_ppl changes to 15\n","num_ppl = 15\n","\n","print(image_paths['name'].value_counts()[:num_ppl])\n","list_people = list(image_paths['name'].value_counts()[:num_ppl].keys())\n","list_num_images = list(image_paths['name'].value_counts()[:num_ppl])\n","print(list_people, list_num_images)\n","\n","num_for_each = image_paths['name'].value_counts()[num_ppl-1]\n","tmp_train = []\n","tmp_val = []\n","tmp_test = []\n","for name in list(image_paths['name'].value_counts()[:num_ppl].keys()):\n","    data_all = image_paths[image_paths.name==name].sample(num_for_each)\n","    data_train, data_test = train_test_split(data_all, test_size=0.2)\n","    data_train, data_val = train_test_split(data_train, test_size=0.2)\n","    tmp_train.append(data_train.copy())\n","    tmp_val.append(data_val.copy())\n","    tmp_test.append(data_test.copy())\n","data_train = pd.concat(tmp_train)\n","data_val = pd.concat(tmp_val)\n","data_test = pd.concat(tmp_test)\n","print(data_train.shape, data_val.shape, data_test.shape)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_root = './data2/'\n","\n","data_list = [data_train, data_val, data_test]\n","dirs = ['train', 'val', 'test']\n","\n","# \"\"\"             # (un)comment this line (only) and run, to copy\n","\n","# # remove data directory if it exists\n","if os.path.exists(data_root) and os.path.isdir(data_root):\n","    shutil.rmtree(data_root)\n","\n","transform_augment = transforms.Compose([\n","    transforms.RandomHorizontalFlip(p=1)\n","])\n","\n","for i in range(len(dirs)):\n","    pathlib.Path(os.path.join(data_root, dirs[i])).mkdir(parents=True, exist_ok=True)\n","    \n","    data = data_list[i]\n","\n","    for person in list_people:\n","        if len(data_train[data_train['name']==person])>0:\n","            pathlib.Path(os.path.join(data_root, dirs[i], person)).mkdir(parents=True, exist_ok=True)\n","\n","    for im_path in data_list[i].image_path:\n","        name = data[data['image_path']==im_path]['name'].iloc[0]\n","        path_from = os.path.join(data_folder+'/lfw-deepfunneled/', im_path)\n","        filename, file_extension = os.path.splitext(path_from.split('/')[-1])\n","        path_to = os.path.join(data_root, dirs[i], name)\n","\n","        if not os.path.isfile(os.path.join(path_to, im_path)):\n","            shutil.copy(path_from, path_to)         # earlier (just copies image)\n","            \n","            # if dirs[i]!='test':                   # test-time augmentation too?\n","            img = Image.open(path_from)\n","            img = transform_augment(img)            # transformed image\n","            img.save(path_to+'/'+filename+'_transformed'+file_extension)"]},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","FaceCNN                                  [1, 10]                   --\n","├─Sequential: 1-1                        [1, 64]                   --\n","│    └─Conv2d: 2-1                       [1, 64, 123, 123]         9,472\n","│    └─BatchNorm2d: 2-2                  [1, 64, 123, 123]         128\n","│    └─ReLU: 2-3                         [1, 64, 123, 123]         --\n","│    └─MaxPool2d: 2-4                    [1, 64, 61, 61]           --\n","│    └─Dropout: 2-5                      [1, 64, 61, 61]           --\n","│    └─Conv2d: 2-6                       [1, 128, 61, 61]          73,856\n","│    └─BatchNorm2d: 2-7                  [1, 128, 61, 61]          256\n","│    └─ReLU: 2-8                         [1, 128, 61, 61]          --\n","│    └─MaxPool2d: 2-9                    [1, 128, 30, 30]          --\n","│    └─Dropout: 2-10                     [1, 128, 30, 30]          --\n","│    └─Conv2d: 2-11                      [1, 256, 30, 30]          295,168\n","│    └─BatchNorm2d: 2-12                 [1, 256, 30, 30]          512\n","│    └─ReLU: 2-13                        [1, 256, 30, 30]          --\n","│    └─MaxPool2d: 2-14                   [1, 256, 15, 15]          --\n","│    └─Dropout: 2-15                     [1, 256, 15, 15]          --\n","│    └─Conv2d: 2-16                      [1, 64, 15, 15]           147,520\n","│    └─BatchNorm2d: 2-17                 [1, 64, 15, 15]           128\n","│    └─ReLU: 2-18                        [1, 64, 15, 15]           --\n","│    └─Dropout: 2-19                     [1, 64, 15, 15]           --\n","│    └─Flatten: 2-20                     [1, 14400]                --\n","│    └─Linear: 2-21                      [1, 1024]                 14,746,624\n","│    └─ReLU: 2-22                        [1, 1024]                 --\n","│    └─Dropout: 2-23                     [1, 1024]                 --\n","│    └─Linear: 2-24                      [1, 64]                   65,600\n","│    └─ReLU: 2-25                        [1, 64]                   --\n","├─Sequential: 1-2                        [1, 10]                   --\n","│    └─Dropout: 2-26                     [1, 64]                   --\n","│    └─Linear: 2-27                      [1, 10]                   650\n","==========================================================================================\n","Total params: 15,339,914\n","Trainable params: 15,339,914\n","Non-trainable params: 0\n","Total mult-adds (M): 731.78\n","==========================================================================================\n","Input size (MB): 0.75\n","Forward/backward pass size (MB): 27.04\n","Params size (MB): 61.36\n","Estimated Total Size (MB): 89.15\n","==========================================================================================\n","decoder.1.weight\n","decoder.1.bias\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","True\n","True\n"]}],"source":["model.load_state_dict(torch.load('best.model'))\n","model.eval()\n","\n","num_input_channels = 3\n","print(summary(model, input_size=(dataloader_kwargs['batch_size'], num_input_channels, 250, 250)))\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3)\n","loss_fn = nn.CrossEntropyLoss()\n","num_epochs = 30\n","\n","for name, param in model.named_parameters():\n","    if param.requires_grad and 'encoder' in name:\n","        param.requires_grad = False\n","        \n","for name, param in model.named_parameters():\n","    if param.requires_grad:print(name)\n","\n","model.decoder = nn.Sequential(        \n","            nn.Dropout(p=0.5),\n","            nn.Linear(in_features=64, out_features=num_ppl),\n","        )\n","\n","for param in model.parameters():\n","    print(param.requires_grad)\n","    \n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3)\n","loss_fn = nn.CrossEntropyLoss()\n"]},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[],"source":["train_path = os.path.join(data_root, dirs[0])\n","val_path = os.path.join(data_root, dirs[1])\n","test_path = os.path.join(data_root, dirs[2])\n","\n","train_transform = transforms.Compose(transforms=[\n","    # transforms.RandomHorizontalFlip(),\n","    # transforms.Grayscale(num_output_channels=1),         # convert to grayscale\n","    transforms.ToTensor(),\n","    # transforms.Normalize(mean=0, std=255),      # output = (input-mean)/std\n","])\n","test_transform = transforms.Compose(transforms=[\n","    # transforms.Grayscale(num_output_channels=1),         # convert to grayscale\n","    transforms.ToTensor(),\n","    # transforms.Normalize(mean=0, std=255)\n","])\n","\n","dataloader_kwargs = {\n","    'pin_memory': True,\n","    'num_workers': 1,\n","    'batch_size': 1,\n","    'shuffle': True\n","}\n","dataloader_kwargs_triplet = {\n","    'pin_memory': True,\n","    'num_workers': 1,\n","    'batch_size': 4,\n","    'shuffle': True\n","}\n","non_blocking = dataloader_kwargs['pin_memory']  # https://stackoverflow.com/questions/55563376/\n","\n","train_loader = DataLoader(\n","    torchvision.datasets.ImageFolder(train_path, train_transform), **dataloader_kwargs\n",")\n","train_loader_triplet = DataLoader(\n","    torchvision.datasets.ImageFolder(train_path, train_transform), **dataloader_kwargs_triplet\n",")\n","val_loader = DataLoader(\n","    torchvision.datasets.ImageFolder(val_path, test_transform), **dataloader_kwargs\n",")\n","test_loader = DataLoader(\n","    torchvision.datasets.ImageFolder(test_path, test_transform), **dataloader_kwargs\n",")"]},{"cell_type":"code","execution_count":187,"metadata":{},"outputs":[],"source":["def train():\n","    best_acc = 0.0\n","    \n","    for epoch in range(num_epochs):\n","        train_score = 0\n","        cnt = 0\n","        CE_loss = 0\n","        triplet_loss = 0\n","\n","        model.train()\n","        \n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","\n","            optimizer.zero_grad()\n","            \n","            outputs,_ = model(inputs)\n","            \n","            # print(outputs, labels)\n","\n","            loss = loss_fn(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            CE_loss += loss.item()\n","\n","            _, preds = torch.max(outputs.data, 1)\n","            train_score += float(torch.sum(preds==labels.data))\n","            cnt += inputs.shape[0]\n","\n","            # print(preds, labels)\n","\n","\n","        train_acc = train_score/cnt\n","        val_acc = evaluate(val_loader, model)\n","        \n","        print(\"Epoch:\", epoch, \"\\tLoss:\", CE_loss, \"\\tTraining Acc:\", train_acc, \"\\tVal Acc:\", val_acc)\n","\n","        if val_acc > best_acc:\n","            torch.save(model.state_dict(),'best.model.2')\n","            best_acc = val_acc"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train()"]},{"cell_type":"code","execution_count":189,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8\n"]}],"source":["model.load_state_dict(torch.load('best.model.2'))\n","model.eval()\n","score = 0\n","cnt = 0\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","        output, encoded = model(inputs)\n","        _, pred = torch.max(output.data, 1)\n","        score += float(torch.sum(pred==labels.data))\n","        cnt += data[0].shape[0]\n","\n","print(score/cnt)"]},{"cell_type":"markdown","metadata":{},"source":["## Adversarial attack"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self, input_shape):\n","        super(Generator, self).__init__()\n","        self.noise = nn.Parameter(torch.randn(size = input_shape))\n","        # We attempt a simple strategy of adding constant noise to each image\n","\n","    def forward(self, input):\n","        out = input + self.noise\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.load_state_dict(torch.load('best.model'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generator = Generator([1,3,250,250])\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(generator.parameters(), lr=1e-3, weight_decay=1e-3)"]},{"cell_type":"markdown","metadata":{},"source":["### l2 regularized"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def regularizer(image1, image2, lam = 1):\n","    return torch.mean((image1-image2)**2)*lam"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_epochs = 6"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(num_epochs):\n","    train_score = 0\n","    cnt = 0\n","    train_loss = 0\n","    generator.train()\n","    \n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","        optimizer.zero_grad()\n","        gen = generator(inputs)\n","        outputs,_ = model(gen)\n","        \n","        # print(outputs, labels)\n","        loss = -loss_fn(outputs, labels) + regularizer(inputs, gen, lam=2*1e4)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_loss += loss.item()\n","        _, preds = torch.max(outputs.data, 1)\n","        train_score += float(torch.sum(preds==labels.data))\n","        cnt += inputs.shape[0]\n","        \n","    train_acc = train_score/cnt\n","    score = 0\n","    cnt = 0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","            output, encoded = model(generator(inputs))\n","            _, pred = torch.max(output.data, 1)\n","            score += float(torch.sum(pred==labels.data))\n","            cnt += data[0].shape[0]\n","\n","    val_acc = (score/cnt)\n","    print(\"Epoch:\", epoch, \"\\tLoss:\", train_loss, \"\\tTraining Acc:\", train_acc, \"\\tValidation Acc:\", val_acc)\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            plt.figure()\n","            f, axarr = plt.subplots(1,2)\n","            axarr[0].imshow(inputs[0].permute(1, 2, 0))\n","            axarr[1].imshow(generator(inputs)[0].permute(1, 2, 0).detach().numpy())\n","            plt.show(block=True)\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(generator.state_dict(),'generator2')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["score = 0\n","cnt = 0\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","        output, encoded = model(generator(inputs))\n","        _, pred = torch.max(output.data, 1)\n","        score += float(torch.sum(pred==labels.data))\n","        cnt += data[0].shape[0]\n","\n","print(\"test acc:\", score/cnt)\n","for inputs, labels in test_loader:\n","    plt.figure()\n","    f, axarr = plt.subplots(1,2)\n","    axarr[0].imshow(inputs[0].permute(1, 2, 0))\n","    axarr[1].imshow(generator(inputs)[0].permute(1, 2, 0).detach().numpy())\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["### l1 regularized"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def regularizer(image1, image2, lam = 1):\n","    return torch.mean(torch.abs(image1-image2))*lam"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_epochs = 6"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(num_epochs):\n","    train_score = 0\n","    cnt = 0\n","    train_loss = 0\n","    generator.train()\n","    \n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","        optimizer.zero_grad()\n","        gen = generator(inputs)\n","        outputs,_ = model(gen)\n","        \n","        # print(outputs, labels)\n","        loss = -loss_fn(outputs, labels) + regularizer(inputs, gen, lam=1e2)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_loss += loss.item()\n","        _, preds = torch.max(outputs.data, 1)\n","        train_score += float(torch.sum(preds==labels.data))\n","        cnt += inputs.shape[0]\n","        \n","    train_acc = train_score/cnt\n","    score = 0\n","    cnt = 0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","            output, encoded = model(generator(inputs))\n","            _, pred = torch.max(output.data, 1)\n","            score += float(torch.sum(pred==labels.data))\n","            cnt += data[0].shape[0]\n","\n","    val_acc = (score/cnt)\n","    print(\"Epoch:\", epoch, \"\\tLoss:\", train_loss, \"\\tTraining Acc:\", train_acc, \"\\tValidation Acc:\", val_acc)\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            plt.figure()\n","            f, axarr = plt.subplots(1,2)\n","            axarr[0].imshow(inputs[0].permute(1, 2, 0))\n","            axarr[1].imshow(generator(inputs)[0].permute(1, 2, 0).detach().numpy())\n","            plt.show(block=True)\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(generator.state_dict(),'generator1')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["score = 0\n","cnt = 0\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device, non_blocking=non_blocking), labels.to(device, non_blocking=non_blocking)\n","        output, encoded = model(generator(inputs))\n","        _, pred = torch.max(output.data, 1)\n","        score += float(torch.sum(pred==labels.data))\n","        cnt += data[0].shape[0]\n","\n","print(\"test acc:\", score/cnt)\n","for inputs, labels in test_loader:\n","    plt.figure()\n","    f, axarr = plt.subplots(1,2)\n","    axarr[0].imshow(inputs[0].permute(1, 2, 0))\n","    axarr[1].imshow(generator(inputs)[0].permute(1, 2, 0).detach().numpy())\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["### Observations\n","l2 regularized attacks seem to fair better than l1 regularized at visual similarity i.e. need to add \"less\" noise for similar reduction in model accuracy"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"vscode":{"interpreter":{"hash":"8ffc022e556dbf9e4707e0813792d41f1f0550f46106b79e6c9a363a1f17dd45"}}},"nbformat":4,"nbformat_minor":4}
